\documentclass[a4paper, 11pt]{article}
\topmargin-2.0cm

\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{pagecounting}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{lipsum}
\usepackage{enumerate}
%\usepackage{tipa}
%\usepackage{gb4e}
\usepackage{graphicx}
\usepackage{amsmath, amsthm}
%\usepackage{colortbl}
\usepackage{caption}
\usepackage[T1]{fontenc}
\usepackage[a4paper]{geometry}
\usepackage{courier}
\usepackage{listings}
\lstset{
         basicstyle=\footnotesize\ttfamily, 
         numberstyle=\tiny,          
         numbersep=5pt,             
         tabsize=2,                
         extendedchars=true,      
         breaklines=true,        
         showspaces=false,      
         showtabs=false,       
         xleftmargin=17pt,
         framexleftmargin=17pt,
         framexrightmargin=5pt,
         framexbottommargin=4pt,
         showstringspaces=false 
 }
 \lstloadlanguages{
         Haskell
 }

\usepackage{url}

\advance\oddsidemargin-0.35in
\advance\evensidemargin-0.65in
\textheight9.5in
\textwidth6.5in

\newcommand{\hsp}{\hspace*{\parindent}}
\definecolor{gray}{rgb}{0.4,0.4,0.4}

\newcommand\blfootnote[1]{
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}
  \addtocounter{footnote}{-1}
  \endgroup
}

%\sectionfont{\large}

\begin{document}
\lstset{language=Haskell}
\pagestyle{fancy}
\lhead{\textcolor{gray}{Inari Listenmaa}}
\chead{\textcolor{gray}{\bf Grammar Formalisms}}
\rhead{\textcolor{gray}{LP2 2014}}
\lfoot{\textcolor{gray}{}}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{0.5pt} 
\renewcommand{\footrulewidth}{0.5pt} 
\fancyfoot[C]{\footnotesize \textcolor{gray}{}} 

\centerline{ {\Large \bf Constraint Grammar} }
\vspace*{0.2cm}

\section{Introduction}

In this course paper, I describe the implementation of Constraint Grammar in Haskell, using a SAT solver to find a solution that fits all the constraints.

First I describe the existing implementations, followed by my own SAT-based solution.

\section{Constraint Grammar}

Constraint Grammar (CG) was first introduced by \cite{KarlssonTODO}. 
It a tool for disambiguating output by morphological analyser.
CG can run on unrestricted text, given that the morphological analyser
has large vocabulary.

Parsing is an action where an unstructured input is transformed into a
structured output. In a sense, applying CG rules on a text shouldn't
be considered as parsing: it doesn't really build a new structure,
such as phrase structure tree or dependency tree.
The grammatical information is already in the input, and applying the
rules leaves (ideally) only the correct options. 
It doesn't require that the text should be parsed as full units; it is
neither top-down or bottom up. Rules can have a context of specified
amount of words (e.g. 1 to the left; 2 to the right), unlimited amount
of words (e.g. anywhere to the left; 2 to the right and further from
that) or they can be limited by a barrier (e.g. anywhere to the right
until we see an adverb).

In the example below, we show possible tags for the sentence ``the bear sleeps''.

\begin{lstlisting}
the     <Det>
bear    <Noun, Sg>
        <Verb, Pl, Pres>
        <Verb, Inf>
sleep   <Noun, Pl>
        <Verb, Sg3, Pres>
\end{lstlisting}

We could disambiguate this sentence with two rules: 
\begin{enumerate}
\item Select noun (or remove verb) after determiner;
\item Select verb, if is preceded by noun and followed by a sentence boundary.
\end{enumerate}

We would go through the input twice. The first rule would fire at the
first pass: we find a case where there is something tagged as
\texttt{Det} followed by something with \texttt{Noun} among the
analyses. We will apply the rule where we select the noun reading, and
discard the two other options (present tense verb or nonfinite verb).

Then, at the next pass, we have two unambiguous tokens,
and we go through the input again, to see
if the first pass has created chances for new rules to
activate. Now we find a context, where there is an ambiguous word
preceded by a noun (\emph{bear}) and followed by a sentence boundary,
so we can apply rule 2. The finished analysis is as follows:

\begin{lstlisting}
the     <Det>
bear    <Noun, Sg>
sleep   <Verb, Sg3, Pres>
\end{lstlisting}

\subsubsection{Syntax}

``Because syntax, just as morphological disambiguation, relies on
elimination of alternatives, it is advantageous to introduce all
simple syntactic instantiations at once. Readings not having syntactic
labels after morphological analysis get them via morphosyntactic
mapping.''

e.g. it makes sense
7: [Lem "they",Pron,Subj,Nom,P3]
if we have a morph tag, like a pron, to label it already
since we know that they can only be SUBJ
and them can only be OBJ

CG can also be applied to shallow syntax. After having disambiguated
the text morphologically, we can apply possible syntactic labels. For
instance, a noun can have a syntactic role of subject, object or
predicative; and a verb in present tense 3rd person singular can be
only the main verb (possibly combined with a conjunction, e.g. ``the
bear eats and sleeps''). The next step is shown below; the analyses
chosen in the previous step are shown in the angle brackets, and potential
syntactic labels are fully capitalised and preceded by @. The sign >
in the tag \texttt{@DN>} expresses head/modifier relation determiner of the next head nominal to the right

\begin{lstlisting}
the     <Det>            @DN>    
bear    <N, Sg>          @SUBJ
                         @OBJ
                         @PREDC
sleep   <V, Sg3, Pres>   @MAINV
\end{lstlisting}

This set would be disambiguated by a syntactic constraint that would
select subject if followed by a finite verb form.

\cite{koskenniemi92} include also 4 types of clause boundaries (TODO read about
it), such as marking coordinated or subordinated clauses, or showing
beginning and end of an embedded clause.
%; so the parse would be complete if \emph{the} and \emph{bear} would belong to 

(c/p from koskenniemi paper)
``Rules in the CG formalism are typically dedicated
for one of the above tasks, and they are
executed as successive groups.
In finite-state syntax, rules are logically unordered.''




\section{Standard implementation(s)}

The first implementation, CG-1, was introduced in \cite{KarlssonTODO}. Karlsson describes the implementation as ``ad hoc`` and ``without any deeper technical contributions to parsing theory``, and adds that ideally, CG should be implemented with finite state methods.
CG-2, implemented by \cite{tapanainen1996}, used finite state technology. 
The latest version, CG-3, was implemented by \cite{TODO} without finite state technology. The implementation introduced new properties, such as ordering of the rules.

\begin{quote}
[--] the Lisp, C, and C++ implementations of the Constraint Grammar Parser are all fairly ad hoc rule interpreters without any deeper technical contributions to parsing theory. These implementations are not based on the use of well understood and theoretically sound parsing algorithms. Rather, they could be characterized as situation-action parsing programs. 
\end{quote}

\begin{quote}The whole finite-state grammar consists of a set of rules which
constrain the possible choices of word Interpretations, tags and
boundaries to only those which are considered grammatical. The entire grammar Is effectively equivalent to the (theoretical) intersection of all individual rule automata. However, such an intersection would be impractical to compute due to Its huge size.
The logical task for any finite-state parser in the current approach is
to compute the intersection of the unanalyzed sentence automaton and
each rule automaton. Actual parsing can be done in several alternative
ways which are guaranteed to yield the same result, but which vary in
terms of efficiency.
\end{quote}

\subsection{Workflow}

All implementations (TODO check) go through the input many times and apply all possible rules each time. 
After one pass, it may be possible to apply some rules that didn't fire the last time.


\subsection{Resolving conflicts}

For CG-1 and CG-2, ... sections? TODO

In CG-3, the rules are ordered. In case of a conflict, the rule that is described first is given priority.


\section{SAT based implementation}

SAT solving is based on a technique called unit propagation:
a set of complex Boolean clauses is simplified, starting from unit
clauses, which consist of just a single variable, and working up to a
solution, where all variables in the set have a True or False value.

SAT solving is used for many applications where constraints must be
satisfied, such as software and hardware verification.
Application of logic to POS tagging or shallow parsing isn't new;
\cite{lager1998, lager2000} presents POS tagging rules as logic, and uses
a theorem prover (DisLog, TODO cite) to implement small constraint rules.
Inspired by the approach, my implementation tries to emulate the CG
formalism in behaviour and features offered.



\subsection{CG formulated in SAT}

We make each analysis a variable. For instance, the fifth line denotes
a hypothesis that the correct analysis of the 3rd item in the sentence
is \texttt{<V,Sg>}. We call this \texttt{v5}.

\begin{lstlisting}
((1,["the",<det>]),v0)
((2,["bear",<n>,<sg>]),v1)
((2,["bear",<vblex>,<pl>]),v2)
((2,["bear",<vblex>,<fin>]),v3)
((3,["sleep",<n>,<pl>]),v4)
((3,["sleep",<vblex>,<sg>,<p3>]),v5)
\end{lstlisting}


We have defined our variables, but not yet any truth values. We can
start by adding a clause that makes sure at least one variable in each
index is true:
\begin{enumerate}
\item \texttt{[v0]}
\item \texttt{[v1,v2,v3]}
\item \texttt{[v4,v5]}
\end{enumerate}

We could easily add a second rule: for ambiguous tokens, make sure that
only one reading can be true (or a less strict version, where all
readings can't be true). For instance, \texttt{Not (Var 2 $\wedge$ Var
  3)} would be the constraint that the second token can't be both
\texttt{N} and \texttt{V} at the same time. However,
\cite{KarlssonTODO} argues that CG cannot remove genuine ambiguities;
in a case where a sentence is genuinely ambiguous, all readings are
returned.

Another property of CG is that cannot remove all readings: if all
other analyses are eliminated, the remaining analysis must be
returned, even if it were to be eliminated at the next pass. 
This is implemented as a clause that says that at least one literal
of each position must be true.

\subsection{Workflow}

Assume we have a fragment ``both houses and cars''.
The word \emph{both} has analyses \texttt{CoordConj} and
\texttt{Pron}, and \emph{houses} is ambiguous between \texttt{N} and \texttt{V}.
We have in our rule set the following rules:

\begin{enumerate}
\item remove N after Pron
\item select N after *Conj
\item select CoordConj before another CoordConj
\end{enumerate}

In the previous implementations, neither of rules 1 and 2 would fire
before we have disambiguated \emph{both} with rule 3.
In this implementation, we are creating all clauses before giving
them to the SAT solver. We express these constraints as implications:
\begin{itemize}
\item \emph{both} is \texttt{Pron} $\Rightarrow$ remove \texttt{N} for \emph{houses}
\item \emph{both} is \texttt{CoordConj} $\Rightarrow$ select
  \texttt{N} for \emph{houses}
\end{itemize}

After adding this feature, the first rule will give the clause
\texttt{Not (Var 7) :||: Not (Var 6)} and the second \texttt{Not (Var
  7) :||: Var 6}.
This is easily generalised to multiple contexts; \texttt{Not (Var
  7 :&&: Var 8) :||: Var 6}.

For lemmas with only one reading, this leads to redundant clauses,
such as \texttt{Not (Var 8) :||: Not (Var 8)}, but SAT solver will
take care of them.

\subsubsection{Negation}

Say we have a rule that inforces agreement: if token at n-1 is Sg,
choose Sg for n as well.

\begin{lstlisting}
rmPlIfSg = Remove [Pl] (C (Exactly -1) [Sg])
rmSgIfPl = Remove [Sg] (C (Exactly -1) [Pl])
\end{lstlisting}

We could also express the rules like this:
\begin{lstlisting}
rmPlIfSg = Remove [Pl] (C (Exactly (-1)) (False,[Pl]))
rmSgIfPl = Remove [Sg] (C (Exactly (-1)) (False,[Sg]))
\end{lstlisting}

Although, this is bad: 
\begin{lstlisting}
((1,[Lem "the",Det]),Var 1)
((2,[Lem "bear",N,Sg]),Var 2)
((2,[Lem "bear",V,Pl]),Var 3)
((3,[Lem "sleep",N,Pl]),Var 4)
((3,[Lem "sleep",V,Sg,P3]),Var 5)
\end{lstlisting}

Negation removes both readings for token 2 ``bear'', because the first
token doesn't have any number reading.

\subsection{Data types}

\begin{lstlisting}
data Rule = Remove [Tag] Condition | Select [Tag] Condition

data Condition = C Position (Bool,[Tag])
               | AND Condition Condition
               | OR  Condition Condition 
\end{lstlisting}

The data type for Rule is either remove or select a list of tags, with condition(s).
For Condition, there are basic conditions (position and a list of context tags) and combined conditions, with constructors `AND' and `OR'. For instance, the following rule is read as ``remove tag for any verb, if the lemma is \emph{bear} or there is a determiner or an article anywhere to the left''.

\begin{lstlisting}
verb = [V, V2, V3, VV, V2V, VS]
Remove verb (OR (C (Exactly 0)  [Lem "bear"])
                (C (AtLeast -1) [Det,Art])
            )
\end{lstlisting}

There is no special constructor for empty condition (ie. remove/select tag everywhere), but an empty tag list in a condition, i.e. \texttt{C \_ []} is assumed to mean that.

The data type for position can be exact n places or at least n places. The number 0 means the word itself, negative number means \emph{n} positions to the left and positive \emph{n} positions to the right.

\begin{lstlisting}
data Position = Exactly Integer | AtLeast Integer
 \end{lstlisting}

\subsection{Complex conditions}
Conditions constructed by OR are represented as a sequence, and AND as
parallel. In Haskell, I simply use nested lists: everything inside the
outer list is subject to OR, and within the lists AND. This is best
demonstrated by an example:

\begin{itemize}
\item \texttt{OR (OR C1 C2) C3} becomes \texttt{[[C1], [C2], [C3]]} 
\item \texttt{AND (AND C1 C2) C3} becomes \texttt{[[C1, C2, C3]]} 
\item \texttt{AND (OR C1 C2) C3} becomes \texttt{[[C1,C3], [C2,C3]]}
\end{itemize}

\subsection{Ordering}

We had three strategies:
maximise: try to make as many rules true as possible;
maximise from top: apply rules from the beginning, discard rule if it
conflicts with the ones added;
discard from bottom: try to add all rules, if conflict, find which
conflict, and try again with only the rules that are in the list
before the first conflicting rule.

\subsection{Weird things}


mkVars: difference between options 1) and 2)

        mkVars :: [(Token,[Token])] -> (Bit -> Bit) -> [[Bit]]
1)        mkVars tctx f = [ f conseq:(map nt causes) | (t, ts) <- tctx
2)        mkVars tctx f = [ [nt cause,f conseq] | (t, ts) <- tctx
                                               , let conseq = getBit t
                                               , let causes = map getBit ts
                                               , cause <- causes ]


% Remove [[vblex]] (POS (AND (C (Exactly 0) (True,[[vblex,p1],[vblex,p2],[vblex,p3]])) (C (Barrier 1 [[cnjcoo],[cnjsub],[cnjadv],[rel]]) (True,[[vblex,p1],[vblex,p2],[vblex,p3]]))))
% [Lit ~v6,Lit ~v2]

We have rules:
REMOVE:r_como_1 Vblex (0 ("como") OR ("Como")) (*-1 Vblex BARRIER Cnj_Rel) ;
REMOVE:r_verb_1 Vblex (0 Vblex + Pers) (*1C Vblex + Pers BARRIER
Cnj_Rel) ;

And tokens:
((1,[">>>",>>>]),Lit v0)
((2,["<Jared>","*Jared"]),Lit v1)
((3,["<explica>","explicar",vblexfoo,pri,p3,sg]),Lit v2)
((3,["<explica>","explicar",vblex,imp,p2,sg]),Lit v3)
((4,["<como>","como",pr]),Lit v4)
((4,["<como>","como",rel,adv]),Lit v5)
((4,["<como>","comer",vblex,pri,p1,sg]),Lit v6)
((5,["<pequeñas>","pequeño",adj,f,pl]),Lit v7)
((6,["<diferencias>","diferencia",n,f,pl]),Lit v8)
((6,["<diferencias>","diferenciar",vblex,pri,p2,sg]),Lit v9)


In CG3, r_como_1 is applied first, and v6 is eliminated. r_verb_1
never has the Vblex+Pers condition available and is never triggered.
In CGSAT, neither is applied first; r_como_1 is is implication ``v2 =>
not v6'', ``v3 => not v6'' and r_verb_1 is ``v6 => not v2'' ``v6 => not v3''.

You can flip rules!

\begin{verbatim}
SELECT (det "<az>") IF (1 AdjOrN);
SELECT AdjOrN IF (-1 (det "<az>"));

REMOVE ("de" adv) IF (NOT 1C (adj));
REMOVE AllButAdj IF (-1 ("de" adv));
\end{verbatim}

\subsection{Example run}

We show an example run for the sentence \emph{They are both happy and go to the house}. This is
taken from a real output of a morphological analyser, and was
initially ambiguous in the following ways:

\begin{itemize}
\item[] are
\begin{itemize}
\item[] \texttt{<''be'', V, Pres>}
\item[] \texttt{<''are'', N, Sg>}
\end{itemize}

\item[] both
\begin{itemize}
\item[] \texttt{<''both'', CoordConj>}
\item[] \texttt{<''both'',Det>}
\item[] \texttt{<''both'',Pron>}
\end{itemize}


\item[] go
\begin{itemize}
\item[] \texttt{<''go'', V, Inf>}
\item[] \texttt{<''go'', V, Pres>}
\item[] \texttt{<''go'', N, Sg>}
\end{itemize}

\item[] to
\begin{itemize}
\item[] \texttt{<''to'', Prep>}
\item[] \texttt{<''to'', Adv>}
\end{itemize}

\item[] house
\begin{itemize}
\item[] \texttt{<''house'', V, Inf>}
\item[] \texttt{<''house'', V, Pres>}
\item[] \texttt{<''house'', N, Sg>}
\end{itemize}
\end{itemize}

\begin{verbatim}
Tag sequence:
1: [Lem "they",Pron,Subj,Nom,P3]
2: [Lem "are",N,Sg]
3: [Lem "both",Pron]
4: [Lem "happy",Adj]
5: [Lem "and",CoordConj]
6: [Lem "go",V,Inf]
6: [Lem "go",V,Pres]
7: [Lem "to",Prep]
8: [Lem "the",Det]
9: [Lem "house",N,Sg]
\end{verbatim}

%If you want references on a separate page, uncomment the following command.
%\clearpage

%\begin{small}
%\bibliographystyle{plainnat}
%\bibliography{references}
%\end{small}

\end{document}

