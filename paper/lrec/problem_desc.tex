\section{Introduction}
\label{sec:intro}

Constraint Grammar (CG, \cite{karlsson1995constraint})
% \citep[CG][]{karlsson1995constraint} 
is a method for disambiguating morphologically analysed text% and adding syntactic or dependency labels
. 
A grammar consists of rules that target specific analyses for selection or removal, based on contextual tests. For example, the following rule
\begin{itemize}
\item[] \texttt{REMOVE verb IF (-1C det) ;}
\end{itemize}
removes a verb reading from a word which is preceded by an unambiguously (C) tagged determiner.
Given the following text,
\begin{itemize}
\item[] 
\begin{verbatim}
"<the>"
        "the" det def
"<bear>"
        "bear" noun sg
        "bear" verb pl
\end{verbatim}
\end{itemize}
the rule will apply to the word \emph{bear}, and remove the analysis \texttt{verb pl}.
However, if a word has only one remaining analysis, then the rule will not apply, even if the condition is met.
%Rules are not allowed to remove the last remaining analysis: if  \texttt{verb pl} were the only analysis for \emph{bear}, then the rule would not apply, even if the condition is met.

CGs are valuable resources for rule-based NLP, especially for lesser resourced languages. They are robust and can be written without large corpora--only morphological analysis is needed. The formalism is lightweight and language-independent, and resources can be shared between related languages \cite{bick2006spanish}, \cite{lene_trond_linda2010}.
Mature CGs contain some thousands of rules, but even small CGs are shown to be effective \cite{lene_trond2011}.

As CGs grow larger, it gets harder for the grammar writers and users to keep track of all the rules and their interaction.
\cite{voutilainen2004} gives a detailed account about best practices of grammar writing and efficient use of hand-annotated corpora to aid the grammar development.
\cite{bick2013tuning} presents a method to optimise hand-written CGs using machine learning techniques.
Our toolset is designed to complement the existing approaches in CG analysis and optimisation.
While Bick's experiments show improvements in results, the grammar writer is none the wiser why is the grammar better.
Authors who have a large hand-annotated corpus can also benefit from the technique.
With our method, it is possible to ask, for example, the following questions:

\begin{itemize}
\item Does my grammar contain rules that contradict each other?
\item Does my grammar contain rules that will never fire?
\item Generate a sequence that triggers rule(s) X but not rule(s) Y
\item Generate a sequence that is ambiguous but doesn't trigger any rules
\end{itemize}

The technique requires an existing morphological lexicon, but no corpus. 
The lexicon is needed in order to generate sequences that have realistic ambiguities: e.g. a word can be ambiguous between noun and verb, but not between adjective and punctuation. 
Constraints for the structure of the sequences come from the CG rules themselves.
We encode both of these constraints as a satisfiability problem, and use a SAT solver.

The paper is structured as follows. Section~\ref{sec:prev} relates our work to the previous research. Section~\ref{sec:implementation} discusses the techniques we use, Section~X presents preliminary results and  Section~Y discusses future work. Section~Z concludes the paper.

\todo{Relate to conf themes: "Methodologies and tools for LRs construction and annotation" and
"Validation and quality assurance of LRs"}






\section{Previous work}
\label{sec:prev}

\begin{quote}Another desirable facility in the grammar development environment would
be a mechanism for identifying pairs of constraints that contradict each
other.
--Atro Voutilainen, 2004
\end{quote}

This section describes prior research and relates our contribution to the existing approaches.
We combine elements from the following aspects of CG research:

\begin{itemize}
\item Corpus-based methods in manual grammar development \cite{voutilainen2004}
\item Optimising hand-written grammars \cite{bick2013tuning}
\item Encoding CG in logic \cite{lager98}, \cite{lager_nivre01}, \cite{listenmaa_claessen2015}
\end{itemize}

In addition, there is a large body of research on automatically inducing rules, e.g. \cite{inducing_cg1996}, \cite{lindberg_eineborg98ilp}%, \cite{lager01transformation}, \cite{asfrent14} 
. However, since our work is aimed to aid the process of hand-crafting rules, we omit those works from our discussion.


\paragraph{Corpus-based methods in manual grammar development}

Hand-annotated corpora are commonly used in the development of CGs, because they give immediate feedback whether a new rule increases or decreases accuracy, and by how large margin \cite{voutilainen2004}. For a language with no free or tagset-compatible corpus available, \cite{tyers_reynolds2015} describe a method where they apply their rules to unannotated Wikipedia texts and pick 100 examples at random for manual check.

% \begin{itemize}
% \item \# times the rule was applied
% \item \% times it was correct
% \end{itemize}
CG rules are usually arranged in sections, from safest to least safe. Given a grammar that has three sections, the rules are applied to the text in the following manner. First apply rules from section 1, and repeat until nothing changes in the text. Then apply rules from sections 1--2, and finally all rules, from sections 1--3.
The best strategy is to start with the most effective rules with low error rate, so that they make way for the following, more heuristic and less safe rules to act on.

A representative corpus is arguably the best way to get concrete numbers: how many times a rule applied and how often it was correct.
However, this method will not notice a missed opportunity, nor suggest a way to improve, for instance changing the place of a rule.


% \cite{voutilainen2004} state that the around 200 rules are probably enough to resolve 50--75 \% of ambiguities in the corpus used in the development. 
% This is invaluable, especially at the beginning stages of grammar writing. 
% Sometimes you just want to know that a rule fired 500 times and was correct 490 times, rather than ``it is possible to construct a word sequence that will trigger this rule''.

%The connection to the reality has also its limitations. 
% What if instead of firing 500 times, it could have fired 1000 times if it had been placed earlier in the rule sequence.
%Or the rule that was wrong in 60 \% of the time,



\paragraph{Optimising hand-written grammars}
\begin{itemize}
\item Variations in the place of the rule
\item Variations in the conditions (careful, ...) of the rule
\end{itemize}
Good side: try all combinations to find the best result. 
At a certain point, the grammar gets so big that it is hard to keep track of all the rules and their interactions. \cite{bick2013tuning} tries out combinations of moving rules in different sections or removing them in total, and in parallel, making their contexts stricter or less strict. 
This is a valuable tool, especially for grammars that are so big that it's hard to keep track of. Program can try all combinations whereas trying to make sense out of a huge set of rules would be hard for humans.
As a downside, the grammar writer will likely not know why exactly is the tuned grammar performing better.


\paragraph{CG encoded in logic}

\cite{lager98} presents a CG-like shallow parsing system encoded in logic, and \cite{lager_nivre01} continues with a reconstruction of four different formalisms.
%The earlier works on logical reconstruction don't envision grammar analysis as one of the use cases,
Grammar analysis is a natural use case, due to some key features of the logical reconstruction.

The/A(n) 
\todo{imperative/straight-forward/efficient/...?}
CG compiler
%, such as VISL CG-3,
 cannot capture any dependencies between rules.
% It discards analyses immediately: the output of the $i^{th}$ rule becomes the input of the $i+1^{th}$ rule, but there is no information which rules have been applied before.
In contrast, a logic-based CG compiler does that by default. 
The rules are modelled as implications and composed in the order of the rule sequence, such that 
the consequent from the $i^{th}$ rule becomes the antecedent of the $i+1^{th}$ rule.
Given this design, we added on top a way to ask for solutions with certain properties.
%, such as ``after applying rules $0-i$, is it possible for $i+1$ to apply''.

% Application of rules is presented as a composition of clauses

% They model CG as clauses o
% The key elements in the ontology of CG are positions, words and sets of tags.
% Rule order is denoted by predicate $pos^i$(word, [tag]), 
% which denotes the part of speech of a given word after applying the $i^{th}$ rule.
% The rules are modelled as implications, of the form below:

% \begin{itemize}
% \item Interaction of rules
% \item Create word sequences to test what is possible and what can never happen
% \end{itemize}




% * Describe problem: CGs are huge & prone to mistakes,
%  * we are looking at conflicts such as ...
%  * if you forgot a case
%  * some evidence that big CGs have conflicts and this is a real problem
%  * Eckhard's paper to explain why conflicting rules are a problem
%  * Goal: help grammar writer while they are writing the grammar, to avoid these kinds of problems
%  * Examples

% * Describe the technique





