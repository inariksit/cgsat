\section{Introduction}
\label{sec:intro}

% Inari: Added this for my academic writing course, as for general motivation.

%Most of NLP research has been since the 90s aimed towards data-driven methods. While these approaches have produced good results for languages with large annotated corpora, they are practically unusable for low-resourced languages. Even for languages with many resources, \todo{cite Manning (2011)} argues that rule-based methods are needed to push the accuracy of POS taggers from the 97\%s to the near 100\%. POS tagging is a fundamental phase which is required in further syntactic or semantic analysis---hence a good POS-tagger is an important resource for any language. We believe that Constraint Grammar is a good candidate especially for languages with low resources, and in this paper we describe methods for CG writer and users to analyse and improve their grammars.

Constraint Grammar (CG, \newcite{karlsson1995constraint})
% \citep[CG,][]{karlsson1995constraint} 
is a formalism used to disambiguate morphologically analysed text. 
A grammar consists of rules that target specific readings for selection or removal, based on contextual tests. For example, the following rule
\begin{itemize}
\item[] \texttt{REMOVE verb IF (-1 det) ;}
\end{itemize}
removes all verb readings from a word which is preceded by a determiner.
Given the following text,
\begin{itemize}
\item[] 
\begin{verbatim}
"<the>"
        "the" det def
"<wish>"
        "wish" noun sg
        "wish" verb pl
        "wish" verb inf
\end{verbatim}
\end{itemize}
the rule will match to the word \emph{wish}, and remove the readings
\texttt{"wish" verb pl} and \texttt{"wish" verb inf}.
Note that if the target word has only one remaining reading, then the
rule will not apply, even if the condition is met.

CGs are valuable resources for rule-based NLP, especially for lesser
resourced languages. They are robust and can be written without large
corpora---only a morphological analyser is needed. The formalism is
lightweight and language-independent, and resources can be shared
between related languages \cite{bick2006spanish,lene_trond_linda2010}.
Mature CGs contain some thousands of rules, but even small CGs are
shown to be effective \cite{lene_trond2011}.


By design, CG is a shallow and robust formalism. 
There is no particular hierarchy between lexical, morphological,
syntactic or even semantic tags: individual rules can be written to address any
property, such as ``verb'', ``copula verb in first person singular'',
or ``the word form \emph{sailor}, preceded by \emph{drunken} anywhere in the
sentence''. This makes it possible to treat very particular edge
cases without touching the more general rule: we would simply write
the narrow rule first (``if noun AND \emph{sailor}''), and introduce
the general rule (``if noun'') later.


However, this design is not without problems. As CGs grow larger, it
gets harder to keep track of all the rules and their interaction.
Our tool will help grammar writers and users to find conflicting
rules, diagnose problems and improve their grammars. 
We expect two major use cases: 
first, to test the effect of new rules while writing a grammar, and
second, to take a complete grammar and analyse it as a whole, to find
conflicts or dead rules.



\begin{figure}[t]
\centering

\begin{itemize}
\item[]
\begin{verbatim}SELECT Inf IF (-1 Para OR De) (0C V) ;
SELECT Inf IF (-1 Prep) (0C V) ;
SELECT Inf IF (-1C Vai) ;
SELECT Inf IF (-1C Vbmod) (0C V) ;
SELECT Inf IF (-1C Ter/de) ;
SELECT Inf IF (-1C Vbmod) (0 Ser) ;
\end{verbatim}
\end{itemize}

\caption{Rules to select infinitive in Portuguese.}

\label{fig:infrules}
\end{figure}






Given the rules in figures~\ref{fig:infrules}~and~\ref{fig:regroup}, a
grammar writer may ask the following questions while writing a grammar. 

\begin{itemize}
\item Are all the rules distinct? (e.g. \texttt{Para} and \texttt{De} may be included in \texttt{Prep})
\item Can two or more rules be merged? (e.g. \texttt{SELECT Inf IF
    (-1C Prep OR Vai OR Vbmod)})
\item Can a messy rule be rewritten in a neater way without changing the meaning?
\item What is the best order for the rules?
\item Generate a sentence that triggers a given list of rules $R$ but not $R'$ 
\end{itemize}

%%%%%

For the second use case, here are examples of conflicts that our tool will detect.
\begin{itemize}
\item If two equivalent rules $r$ and $r'$ occur in the grammar, the second occurrence will be disabled by the first
\item Rule $r$ selects something in a context, and $r'$ removes~it
\item A list of tules $R$ removes something from the context of a rule $r$, so $r$ can never
  apply
\item A rule $r$ has an internal conflict, such as non-existent
tag combination, or contradictory requirements for a context word
\end{itemize}
In the above examples, $R$ can be a single rule or a list of rules: for instance, if one rule removes a verb in
context $C$, and another in context $\neg C$, together these rules
remove a verb in all possible cases, disabling any future rule that
targets verbs.

While rule-internal conflicts can be detected by simpler means, taking
care of rule interaction requires a {\em semantic} rather than a {\em
 syntactic} analysis.
% In order to find effects of rule interaction, we cannot just apply the
% rules to some actual sentences and hope to find a conflict---corpora
% can be extremely valuable in the development of CG rules, but there
% are languages where corpora are not available, or they may contain
% text from limited domains. 
We must keep track of all the possible sentences after applying each
rule. At each step, we have two options: either the rule fires,
or it does not fire. In case the rule does not fire, we have two
reasons why not: one or more of its conditions does not hold, or its
target is the only remaining analysis. The result is a complex table
of interdependent decisions---some determined by being the target of a
rule, others by being a condition.

We express these constraints as a \emph{Boolean satisfiability problem} (SAT).
A SAT-problem consists of two components: a set of Boolean variables, and a set
of clauses on those variables. For instance, let the set of variables
be $\{a, b\}$ and the formulas $\{a \vee b, \neg{}a\}$. A program called a
\emph{SAT-solver} will try to find a solution, where all the variables
are replaced by a Boolean value. For this particular problem, the
unique solution is $\{a=False, b=True\}$, but it is also possible for a
SAT-problem to have no solution or multiple solutions.
In the case of CG analysis, the solution we build (called the \emph{model}) represents a \emph{sentence} that starts as potentially anything, and is being shaped by all the
rules along the way. This is how we can generate possible inputs, as
well as check if the grammar is internally consistent.

The paper is structured as follows. Section~\ref{sec:prev} relates our
work to previous work. 
Section~\ref{sec:implementation} discusses the implementation, and
Section~\ref{sec:eval} presents preliminary results. 
Section~\ref{sec:conclusion} discusses future work and concludes the paper.


\begin{figure}[t]
\centering
\begin{itemize}

\item[]\texttt{SELECT V + Prs/Imprt + Act + Neg}

\item[\texttt{IF}]
\begin{verbatim}(*-1C Negv LINK NOT *1 Vfin/PrsPrc/Inf) 
(NOT 0 N) (NOT 0 Pron) 
(NOT *1 Neg) (NOT *-1 Neg)
(NOT 0 Pass) (NOT *-1 Niin) 
(*-1C Negv LINK NOT *1 CLB?) 
(*-1C Negv LINK NOT 0 Imprt) ; 
\end{verbatim}

\item[\texttt{IF}]
\begin{verbatim}(NOT *-1 Niin OR Neg)  
(*-1C Negv
  LINK NOT 0 Imprt
  LINK NOT *1 Vfin/PrsPrc/Inf OR CLB?) 
(NOT 0 N OR Pron OR Pass) 
(NOT *1 Neg) ;
\end{verbatim}
\end{itemize}

\caption{Two versions of a condition in Finnish.}

\label{fig:regroup}
\end{figure}

%these possibilities as SAT variables, and ask for a solution: ``after all
%rules in $R$ have applied, is there an input that will make $r'$ fire?'' 
%To our knowledge, there is no other CG tool that would keep track of
%previously executed rules



% They can ask specific questions about a given grammar, such as:
% \begin{itemize}
% \item Are there rules that contradict each other?
% \item Are there rules that will never fire?
% \item Generate a sequence that triggers rule(s) $R$ but not rule(s) $R'$
% %\item Generate a sequence that is ambiguous but doesn't trigger any rules
% \end{itemize}
% Our technique requires an existing morphological lexicon, compatible
% with the tag set used in the grammar, but no corpus. 
% The lexicon is needed in order to limit the possible tag combinations.
% Constraints for the structure of the sequences come from the CG rules themselves.
% We encode both of these constraints as a satisfiability problem, and use a SAT-solver to generate answers to the previous questions.


% Section~\ref{sec:implementation} discusses the implementation, 
% Section~\ref{sec:eval} presents preliminary results and discusses future work. 
%Section~\ref{sec:conclusion} concludes the paper.

%\todo{Relate to conf themes: "Methodologies and tools for LRs construction and annotation" and
%"Validation and quality assurance of LRs"}






\section{Related work}
\label{sec:prev}

% \begin{quote}Another desirable facility in the grammar development environment would
% be a mechanism for identifying pairs of constraints that contradict each
% other.
% --Atro Voutilainen, 2004
% \end{quote}

%This section describes prior research and relates our contribution to the existing approaches.
We combine elements from the following aspects of CG research:

\begin{itemize}
\item Corpus-based methods in manual grammar development \cite{voutilainen2004}
\item Optimising hand-written CGs~\cite{bick2013tuning}
\item Encoding CG in logic \cite{lager98,lager_nivre01,listenmaa_claessen2015}
\end{itemize}

In addition, there is a large body of research on automatically
inducing rules, e.g. \newcite{inducing_cg1996}, \newcite{lindberg_eineborg98ilp},
\newcite{lager01transformation} and \newcite{asfrent14}.
However, since our work is aimed to aid the process of hand-crafting rules, we omit those works from our discussion.


\paragraph{Corpus-based methods in manual grammar development}

Hand-annotated corpora are commonly used in the development of CGs, because they give immediate feedback whether a new rule increases or decreases accuracy \cite{voutilainen2004}.
This helps the grammar writer to arrange the rules in appropriate sections, with safest and most effective rules coming first.
However, this method will not notice a missed opportunity or a grammar-internal conflict, nor suggest ways to improve.

% -- uncomment for full version ?

%\cite{voutilainen2004} gives a detailed account about best practices of grammar writing and efficient use of corpora to aid the grammar development.
%For a language with no free or tagset-compatible corpus available, \cite{tyers_reynolds2015} describe a method where they apply their rules to unannotated Wikipedia texts and pick 100 examples at random for manual check.

% CG rules are usually arranged in sections, and run in the following manner. 
% First apply rules from section 1, and repeat until nothing changes in the text. Then apply rules from sections 1--2, then 1--3 and so on, until the set includes all rules.
% The best strategy is to place the safest and most effective rules in the first sections,
% so that they make way for the following, more heuristic and less safe rules to act on.

% A representative corpus is arguably the best way to get concrete numbers---how many times a rule applied and how often it was correct---and to arrange the rules in sections based on that feedback.
% However, this method will not notice a missed opportunity or a grammar-internal conflict, nor suggest ways to improve.

% \cite{voutilainen2004} state that the around 200 rules are probably enough to resolve 50--75 \% of ambiguities in the corpus used in the development. 



\paragraph{Automatic optimisation of hand-written grammars }
% The corpus-based method can tell the effect of each single rule at their place in the rule sequence, and leaves the grammar writer to make changes in the grammar.

\newcite{bick2013tuning} modifies the grammar automatically, by trying
out different rule orders and altering the contexts of the rules.
Bick reports error reduction of 7--15\% compared to the original grammars.
As a downside, the grammar writer will likely not know why exactly does the tuned grammar perform better.
% At a certain point, the grammar gets so big that it is hard to keep track of all the rules and their interactions. \cite{bick2013tuning} tries out combinations of moving rules in different sections or removing them in total, and in parallel, making their contexts stricter or less strict. 
% This is a valuable tool, especially for grammars that are so big that it's hard to keep track of. Program can try all combinations whereas trying to make sense out of a huge set of rules would be hard for humans.



\paragraph{CG encoded in logic}

%\cite{lager98} presents a CG-like shallow parsing system encoded in logic, and \cite{lager_nivre01} continues with a reconstruction of four different formalisms.
%The earlier works on logical reconstruction don't envision grammar analysis as one of the use cases,
\newcite{lager98} and \newcite{lager_nivre01} reconstruct the CG formalism in first-order predicate logic.
%, and \cite{listenmaa_claessen2015} implement a CG compiler using a SAT-solver.
Grammar analysis is a natural use case, due to some key features of the logical reconstruction.
The traditional CG compiler 
%, such as VISL CG-3,
 cannot capture any dependencies between rules.
% It discards analyses immediately: the output of the $i^{th}$ rule becomes the input of the $i+1^{th}$ rule, but there is no information which rules have been applied before.
In contrast, a logic-based CG compiler does that by default. 
The rules are modelled as implications and composed in the order of the rule sequence, such that 
the consequent from the $i^{th}$ rule becomes the antecedent of the $i+1^{th}$ rule.
Given this design, we added on top a way to ask for solutions with certain properties.
%, such as ``after applying rules $0-i$, is it possible for $i+1$ to apply''.

% The key elements in the ontology of CG are positions, words and sets of tags.
% Rule order is denoted by predicate $pos^i$(word, [tag]), 
% which denotes the part of speech of a given word after applying the $i^{th}$ rule.
% The rules are modelled as implications, of the form below:






% * Describe problem: CGs are huge & prone to mistakes,
%  * we are looking at conflicts such as ...
%  * if you forgot a case
%  * some evidence that big CGs have conflicts and this is a real problem
%  * Eckhard's paper to explain why conflicting rules are a problem
%  * Goal: help grammar writer while they are writing the grammar, to avoid these kinds of problems
%  * Examples

% * Describe the technique





