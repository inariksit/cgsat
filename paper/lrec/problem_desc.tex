\section{Introduction}
\label{sec:intro}

Constraint Grammar (CG, \cite{karlsson1995constraint})
% \citep[CG][]{karlsson1995constraint} 
is a formalism for disambiguating morphologically analysed input.
CGs are valuable language resources for rule-based language processing, especially for lesser resourced languages. They are robust and require no extensive corpora or other language resources. The formalism is lightweight, and even small CGs are shown to be worthwile \cite{lene_trond2011}.

As CGs grow larger, .

\cite{bick2013tuning} presents efforts to optimise hand-written CGs using machine learning techniques.
While Bick's experiments show improvements in results, the grammar writer is none the wiser why is the grammar better.
Our toolset is designed to complement the existing efforts.
The ideal use case would be a grammar writer, who wants to know e.g.

\begin{itemize}
\item Is there a sentence that triggers rule(s) X but not rule(s) Y
\item Give a sentence that is ambiguous but doesn't trigger any rules
\item Does my grammar contain ``dead code'' -- rules that would not fire in any case?
\end{itemize}

The technique requires an existing morphological lexicon, but no corpus.
This makes it suitable also for languages without large corpora.
Authors writing grammars for well-resourced languages can also benefit from the technique, as an additional tool along with a gold standard corpus. 
Hand-annotated corpora are commonly used in the development of CGs, because they give immediate feedback whether a new rule increases or decreases accuracy, and by how large margin (\cite{voutilainen2004}). For a language with no free hand-tagged corpus available, \cite{tyers_reynolds2015} describe 

Things you can get from corpus:

\begin{itemize}
\item \# times the rule was applied
\item \% times it was correct
\end{itemize}
Strengths: gives concrete feedback about the effect of rules. Good especially at the beginning stages of grammar writing.

Things you can get from ML tuning:
\begin{itemize}
\item Variations in the place of the rule
\item Variations in the conditions (careful, ...) of the rule
\end{itemize}
Good side: try all combinations to find the best result.


Things you can get from SAT solver:
\begin{itemize}
\item Interaction of rules; 
\item Create word sequences to test what is possible and what can never happen
\end{itemize}


Given a large morphological lexicon, it is relatively easy to restrict each individual word to contain only realistic ambiguities. We expand the lexicon, map each wordform to its possible analyses, and then ignore the actual wordforms. For instance, a Spanish morphological lexicon contains the entries \texttt{casa:<verb><sg><p3>} and \texttt{casa:<noun><sg>}, hence we know that a confusion between a 3rd person singular verb and a singular noun is possible. The lexicon is not likely to contain a wordform that can be analysed both as an adjective and punctuation, so that kind of ambiguous word is never created.
However, since our method is corpus-free, it does not contain any restrictions as to what words can follow each other. \footnote{Encode morphological lexicon and CG rules in SAT + ask for a sentence that will trigger rules = natural language generation! \:D/}

\todo{Relate to conf themes: "Methodologies and tools for LRs construction and annotation" and
"Validation and quality assurance of LRs"}

% * Describe problem: CGs are huge & prone to mistakes,
%  * we are looking at conflicts such as ...
%  * if you forgot a case
%  * some evidence that big CGs have conflicts and this is a real problem
%  * Eckhard's paper to explain why conflicting rules are a problem
%  * Goal: help grammar writer while they are writing the grammar, to avoid these kinds of problems
%  * Examples

% * Describe the technique

% * Preliminary results
%  - dutch & spanish
%  - mention scalability
%  - talk about size of SAT problem -- give number of SAT clauses for the last rule in the biggest grammar I have

% * Future work
%  - analysing different grammar formalisms
%  - asking different questions
%  - restrict yourself to readings that are actually words



