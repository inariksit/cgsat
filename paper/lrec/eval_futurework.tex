\section{Evaluation}
\label{sec:eval}

We tested three grammars to find conflicting rules: 
Dutch\footnote{\scriptsize{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-nld/apertium-nld.nld.rlx}}},
with 59 rules; 
Spanish\footnote{\scriptsize{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-spa/apertium-spa.spa.rlx}}},
with 279 rules; and 
Finnish\footnote{\scriptsize{\url{https://github.com/flammie/apertium-fin/raw/master/apertium-fin.fin.rlx}}},
with 1185 rules. We left out \textsc{add}, \textsc{map} and other rule
types introduced in CG-3, and only tested \textsc{remove} and \textsc{select} rules.
The results for Dutch and Spanish are shown in table~\ref{table:res},
and the results for Finnish in table~\ref{table:resFin}.

A natural follow-up evaluation would be to compare the performance of the
grammar in the original state, and after removing the conflicts found
by our tool. Unfortunately, we did not have time to perform such
evaluation, and in addition, we only have gold standard corpus (20~000~words) for Spanish.



\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}

\hline
                   & \textsc{nld}  & \textsc{spa}lem  & \textsc{spa}nolem \\ \hline
\# rules           & 59            & 279       & 279     \\ \hline
\# readings        & 336           & 3905      & 1735    \\ \hline
\# true positives  & 7             & n         & m    \\ \hline
\# false positives & 0             & n         & m    \\ \hline
\clock{} with amb. 
           classes & ??s           & + 1h      & 30m-1h   \\ \hline
% \clock{} last rule & ??s           & ??s       & ?min ??s    \\ \hline
\clock{} no amb. 
           classes & 10ish sec       & ??min ??s    & ?h ??min    \\ \hline

%\# variables: last rule & 33915         & 247153            &    \\ \hline

\end{tabular}
\caption{Results for Dutch and Spanish grammars.}
\label{table:res}
\end{table}




The experiments revealed problems in all grammars. For the smaller
grammars, we were able to verify manually that the detected rules were
true positives.
%The results for the Finnish grammar are inconclusive, 
% For the Finnish grammar, we did not have time to
% investigate all of them, but among those we could, we found both true and false negatives.
We did not systematically check for false negatives in any of the
grammars, but we kept track of a number of known tricky cases; mostly
rules with negations and complex set operations.
%While this does not guarantee the lack of false negatives, it indicates some degree of reliability. 
As the tool matures and we add new features, a more in-depth analysis
will be needed.

\subsection{Dutch} The Dutch grammar had two kinds of errors: rule-internal, and rule interaction. As for rule-internal conflicts, one was due to a misspelling in the list definition for personal pronouns, which rendered 5 rules ineffective. The other was about subreadings: the genitive \emph{s} is analysed as a subreading in the Apertium morphological analyser, but it appeared in 2 rules as the main reading. 

There was one genuine conflict with rule interaction, shown below:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE Adv IF (1 N) ;
REMOVE Adv IF (-1 Det) (0 Adj) (1 N) ;
\end{verbatim}
\end{itemize}

These two rules both remove an adverb, but the first one has a broader condition:
remove adverb if followed by a noun. In contrast, the second rule has a stricter condition: only remove adverb if it is preceded by a determiner, the adverb itself is ambiguous with an adjective, and followed by a noun. The problem is that the first rule removes the adverb in all possible cases, and the second will not have any chance to act. If the rules were in the opposite order, then there would be no problem.


%%% Nevermind, this wasn't a conflict, my program was just buggy :-P

%\begin{itemize}
%\item[] 
%\begin{verbatim}
%SELECT DetPosNotZijn IF (1 Noun) ;
%SELECT DetPos IF (-1 (vbser pres p3 sg)) 
%                 (0 "zijn") (1 Noun);
%\end{verbatim}
%\end{itemize}

% SELECT DetPosNotZijn IF (1 Noun);
%If the first rule fires, then the condition of the second rule cannot be true: there can never be a ``zijn'' in the target, because the previous rule has selected everything that is a possessive determiner and not ``zijn''. 
%So, the first rule must not fire. It cannot be because condition does not hold: the condition (1 Noun) is shared with the second rule, and  

We also tested rules individually, in a way that a grammar writer might use our tool when writing new rules.
The following rule was one of them:

\begin{itemize}
\item[] 
\texttt{SELECT DetPos IF (-1 (vbser pres p3 sg)) (0 "zijn") (1 Noun);}
\end{itemize} 

As per VISL CG-3, the condition \texttt{(0 "zijn")} does not require
that the input would have determiner and \emph{zijn} in the same reading:
just that there is a reading with any determiner, and a reading with any \emph{zijn} in the cohort. 
Can we catch this imprecise formulation with our tool? The example word constructed by the SAT-solver may as likely be one of the following:

\begin{itemize}
\item[a.] \begin{verbatim}
"w2"
    w2<det><pos><f><sg>
    zijn<vbser><inf>
\end{verbatim}

\item[b.] \begin{verbatim}
"w2"
    zijn<det><pos><mfn><pl>
\end{verbatim}
\end{itemize}

Incidentally, the first example of a rule conflict had a similar
requirement, but used correctly: the intended semantics was indeed
that the adjective and the adverb are in different readings. However,
in this case, the precise way to express the target is \texttt{SELECT DetPos + "zijn"}.

We found the same kind of definitions in many other rules and grammars.
To catch them more systematically, we could add a feature that alerts in all cases where a condition with 0 is used. As a possible extension, we could automatically merge the 0-condition into the target reading, then show the user this new version, along with the original, and ask which one was intended.

% In the latter case, the grammar writer would hopefully notice the imprecise definition and change their definition. However, this is more of a happy side effect than intended feature---the grammar writer cannot count on our tool to notice all similar cases.

\subsection{Spanish} The Spanish grammar had proportionately the
highest number of errors. The grammar we ran is like the one
found in the Apertium repository, apart from two changes: we fixed
some typos (capital O for 0) in order to make it compile, and
commented out two rules that used regular expressions, because our
solution does not handle them properly; see more on Section~\ref{sec:conclusion}.

Again, we can classify the conflicts into internal and interaction. As an example of internal conflict, there are \todo{n} rules that use \texttt{SET Cog = (np cog)}---the problem is that the tag \texttt{cog} does not exist in the Apertium dictionary for Spanish. It is likely that this grammar has been written for an earlier version, where such tag has been in place.


The Spanish grammar has a high number of interaction conflicts, \todo{n} with the same pattern:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE (a) IF (-1 x) (1 y) ;
REMOVE (a) IF (-1 x) (1 y) (NOT -1 z);
\end{verbatim}
\end{itemize}


This is another instance of more general rule applying before more
specific. Negated context works as well to make a rule stricter. The
second rule will never apply, because the first will catch all
cases. For a full list of found conflicts, see \todo{\url{actual://link.here/spanish/error.log}}

In addition, there was a number of set definitions that were never
used. Since VISL CG-3 already points out unused sets, we did not add such
feature in our tool. However, we noticed an unexpected benefit when
we tried to use the set definitions from the grammar directly as our
readings: this way, we can discover inconsistencies even in
set definitions that are not used in any rule.
%If a set definition is impossible, using it in a rule makes the rule have an internal conflict.
For instance, the following definition requires the word to be all of
the listed parts of speech at the same time---the grammar writer meant
OR instead of +:
\begin{itemize}
\item[] 
\texttt{SET NP\_Member = N + A + Det + PreAdv + Adv + Pron ;}
\end{itemize}

If it was used in any rule, that rule would have been marked as
conflicting. We noticed the error by accident, when the program
offered the reading \texttt{w2<n><np><adj><det><preadv><adv><prn>}
in an example sequence meant for another rule.


As with the Dutch grammar, we ran the tool on individual rules and
examined the sequences that were generated. None of the following was
marked as a conflict, but looking at the output indicated that there
are multiple interpretations, such as whether two analyses for a
context word should be in the same reading or different readings.
We observed also cases where the grammar writer has specified desired
behaviour in comments, but the rule does not do what the grammar
writer intended. 

% \begin{itemize}
% \item[] 
% \begin{verbatim}
% REMOVE Vblex IF (0 "como") (*-1 Vblex BARRIER Cnj_Rel) ;
% \end{verbatim}
% \end{itemize}

% Based on the comments in the file, the grammar writer wants to
% disambiguate the word form ``como'', which can be either a
% preposition, adverb or the first person singular of the verb
% ``comer''. 
% However, the rule here addresses the \emph{lemma} ``como'' instead of the
% word form, hence the SAT-solver cannot create a solution where
% \texttt{vblex} and ``como'' as a lemma are in the same
% reading. 
% A precise definition for this rule would target \texttt{vblex "<como>"} 
% or \texttt{vblex "comer"}, so that it becomes obvious that those two
% are supposed to be in the same reading. 

% Currently, as all the word forms are allowed to combine with anything,
% the ambiguity class constraint does not point this
% The automated 0-to-target conversion could come in handy here too.
% However, if the 

% TODO An example of the same but in condition. that the ambiguity class constraint, when
% implemented properly, should find out.



\begin{itemize}
\item[] 
\texttt{REMOVE Podar  IF (0 Podar) (1 Adv) (2C Inf OR Enc) ;} \\
\texttt{REMOVE Sentar IF (0 Sentar) (1C Inf OR Enc) ;}
\end{itemize}

The comments make it clear that the first rule is meant to disambiguate between \emph{poder} and \emph{podar}, and the second rule between \emph{sentir} and \emph{sentar}. But the rule does not mention anything about \emph{poder} nor \emph{sentir}; without the ambiguity class constraints, the SAT-solver could give a triggering sequence with any other reading in the target.

This example shows the 
shows a shortcoming in our way of handling the lexical conditions. 
If the lemmas and word forms were integrated to the analyses in a more
sensible way, then the ambiguity class constraints would create a
sequence where \emph{podar} was ambiguous with \emph{poder} only in the
word form \emph{puedo}.

instead of randomly chosen readings. So, even though this was not marked as a conflict, we can say that the weird output of the tool was unnecessary, and by better implementation, the tool would have created more realistic example sequence.


\begin{table*}[t!]
\centering
\begin{tabular}{|p{2.5cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}

\hline
              & 1 clitic + lemmas from grammar & 2 clitics + lemmas from grammar & 1 clitic + all readings 
                                                from grammar & all readings from grammar \\ \hline
\# readings   & 5851 (4263 
                 + 1588) & 9494
                       (7906 + 1588) & 6657 (4263 + 2394) & 2394  \\ \hline
\# conflicts  & 214      & 214       & 24                & 24  \\ \hline
\# interaction & 214      & 214       & 24                & 24  \\ \hline

\# true 
   positives  & n        & m         & o                & p  \\ \hline
\# false 
   positives  & p        & q         & r                & s   \\ \hline
\clock{}all 
       rules & 12min     & 23min     & 12min &  4min 40s \\ \hline


\end{tabular}
\caption{Results for Finnish (1185 rules).}
\label{table:resFin}
\end{table*}

\subsection{Finnish} 

The results for the Finnish grammar are shown separately, in table~\ref{table:resFin}. We encountered a number of difficulties and used a few shortcuts, which we did not need for the other grammars---most importantly, not using the ambiguity class constraints. Due to these complications, the results are not directly comparable, but we include Finnish in any case, 
to give an idea how our method scales up: both to grammars with more rules, and to more complex rules.

\paragraph{Challenges with Finnish} The first challenge is the morphological complexity of Finnish.
There are more than 20,000 readings, when all possible clitic combinations are included.
After weeding out the most uncommon combinations, we ended up with sets of 4000--8000 readings.

The second challenge comes from the larger size of the grammar. Whereas the Spanish and Dutch had only tens of word forms or lemmas, the Finnish grammar specifies around 900 of them.
Due to both of these factors, the procedure described in Section~\ref{sec:realistic_readings} would have exploded the number of readings, so we simply took the lemmas and word forms, and added them as single readings. 
In cases where they were combined with another tag in the grammar, we took that reading directly: for instance, we took both \emph{aika} and ``\emph{aika} n'' from \texttt{SELECT "aika" + N},
but nothing from \texttt{SELECT Pron + Sg}. This method gave us 1588 additional readings.

The third were not able to create ambiguity class constraints---expanding the Finnish morphological lexicon results in \todo{100s of gigabytes?} of word forms, which is simply too big for our method to work. 
For future development, we will see if it is possible to manipulate the finite automata directly, instead of relying on the output in text.


\paragraph{Results}
The results are shown in table~\ref{table:resFin}.
In the first column, we included only possessive suffixes. In the second column, we included question clitics as well.
In the third column, we included all the tag combinations specified in the grammar, and in the fourth, we took only those, ignoring the morphological lexicon completely.

For the first two columns, there are a high number of internal conflicts. 
These are almost all due to nonexisting tags. The grammar was written in 1995, and updated by \newcite{pirinen2015}; but this high number of internal conflicts indicates that possibly something has gone wrong in the conversion, or in our expansion of the morphological lexicon.
As for accuracy, adding the question clitics did not change anything: they were already included in some of the 1588 sets with word forms or lemmas, and that was enough for the SAT-solver to find models with question clitics.
We left the result in the table just to demonstrate the change in speed.

%Example reading that adds question clitic to our arsenal: \texttt{SELECT "joka" + Pron + Q}

The second two columns are playing with the full set of readings from the grammar. For both of these, the number of reported conflicts was only 24.






To show some examples of true positives:

%SELECT ("oma" gen) IF (NOT 1 n|adj|vblex|vaux|pron|cs|cc|adp|post|pr|intj|num|abbr) (0 LINK *-1 "olla"|"voida"|"saattaa") (0 n|adj|vblex|vaux|pron|cs|cc|adp|post|pr|intj|num|abbr) (0C nom)

\begin{itemize}
\item[] \todo{Put back the rule with 0 gen and 0C nom!!! that was actually nice and legit}

\item[]\texttt{SELECT ADV (-1C AD) (NOT 0 PP) (NOT 1 A) ;}
\end{itemize}

This is an internal conflict, which is not trivial to see: the problem is that the definition of \texttt{PP} includes \texttt{adv} among others. With the sets expanded, this rule becomes \texttt{SELECT adv IF (NOT 0 pp|adv|adp|po|pr) ...}.

The following two examples are interaction conflicts:

\begin{itemize}
\item[]\begin{verbatim}
REMOVE A (0 DER) ; 
REMOVE N (0 DER) ; 
REMOVE A/N (0 DER) ; 
\end{verbatim}
\end{itemize}

In this case, two rules together remove the target of the third, leaving no way for there to be adjective or noun.

\begin{itemize}
\item[]\begin{verbatim}
SELECT:r1 .. IF (-1 Comma/N/Pron/Q) ;
SELECT:r2 .. IF (-2 ..) (-1 Comma) ;
\end{verbatim}
\end{itemize}

The rules above have been simplified to show only the relevant part.
The conflict lies in the fact that \texttt{Comma} is a subset of \texttt{Comma/N/Pron/Q}:
there is no way to trigger $r2$ without placing a comma in position -1, and thereby triggering $r1$.


% All in all, using readings from grammar directly may be useful in order to find just the , sometimes you can have a typo, like in \emph{l\"{a}hetii}:

% \begin{verbatim}
% "w2"
%         w2<"se"><ela>
%         w2<"<lähetii>"><vblex>
% \end{verbatim}


% The second part concerns the tools in use, specifically the Apertium morphological lexicon. We noticed, only after running the experiments, that the expansion of the lexicon does not work properly: for instance, some nominal forms of the verbs were reported as nouns instead of verbs.
% We reported the errors to the developers of the lexicon, but as there was no time to fix them, we used the readings that we got from expanding the lexicon, and manually added either noun, verb or adjective tag where it was missing.

% kosimisiin:kosia<n><pl><ill>
% kosimattomuudekseen:kosia<sg><tra><pxsp3> 

% Secondly, we added ambiguity class constraints for only
% \emph{intraparadigmatic ambiguities}: homonymous wordforms belonging
% to the same lexeme, in a way that is systematic to the whole
% paradigm. We reduced the morphological lexicon to contain only one
% example of each inflection paradigm, and then expanded it: with all
% the clitics, this resulted in \todo{n} word forms. Then we applied the
% normal method of creating the ambiguity classes, and specified some
% default cases manually, such as punctuation and word boundaries.



\subsection{Performance} 
The running time of the grammars goes from seconds to hours. 
As can be seen from the times, increasing the number of readings
matters more than increasing the number of rules.
In addition, the ambiguity class constraints take a long time. 

However, many of the use cases do not require running the whole
grammar. Testing the interaction between 5--10 rules takes just
seconds, regardless of the language. 

%We believe we can still signifcantly improve the performance of the SAT-encoding. Currently it is just a naive proof-of-concept implementation, and we have not tried to optimise for speed at all.

% A Finnish symbolic word has around 5700 variables in the beginning of the analysis, each representing a possible tag combination.
% Around 3700 come from the Apertium morphological lexicon, excluding clitics, and around 2000 are defined in the grammar, as a target (\texttt{REMOVE ("bear" verb) IF ...}) or as a condition (\texttt{REMOVE ... IF (-1 ("bear" noun))}. The majority of them are lexical, whereas we don't include anything but morphological tags from the lexicon.
% For Spanish and Dutch, there are around 2000 and 300 tag combinations respectively.
% The Spanish lexicon includes up to two clitics (e.g. \emph{dámelo} `give.me.it') in the same word form, but Dutch has none in the lexicon.

% The SAT problem for checking one rule grows depending on how many rules come before it, how long the context is, and how many tag combinations are there. The running time of the last rule gives some indication ...


% Not all questions are so expensive.
% For instance, checking the tag sets in the Finnish grammar doesn't require the context of the other rules at all,
%  The majority of the correctly detected errors in the Finnish grammar were rule-internal---these errors don't need the context of the other rules at all.


\section{Conclusions and Future Work}
\label{sec:conclusion}

We set out to design and implement an automatic analysis of constraint grammars that can find problematic rules and rule combinations, without the need for a corpus.
Our evaluation indicates that the tool indeed finds non-trivial conflicts and dead rules
from actual grammars. 

We did not have a volunteer to test the tool in
the process of grammar writing, so we cannot conclude whether the
constructed examples are useful for getting new insights on the rules.

Also, there are still a number of features to improve and add.

\paragraph{Combining morphological and lexical tags}

Our solution to hardcode the tag combinations in the readings is
feasible for simple morphology, but it can cause problems with more
complex morphology.
We are currently working on adding the concepts of lemmas and word
forms directly to the representation of the possible readings.
Currently, if we want to add one more lemma for a verb, we need to
create as many new variables as there are distinct verb forms---easily
hundreds for languages with rich morphology. (INARI: Do you really mean to say 'verb forms' here?)

This may also be feasible to support regular expressions, in a nicer
way than just spitting out the regex. (INARI: You said MORE ON THAT here, good because I don't understand what you want to say here :)

\paragraph{Full expressivity of CG-3}
As for longer-term goals, we want to handle the full expressivity of CG-3,
with \textsc{map}, \textsc{add},  \textsc{addreading} and
\textsc{substitute} rules, as well as dependency structure. 
This also means finding different kinds of conflicts.
In order to implement rules that may add new readings, or new tags to
existing readings, we need to modify our approach in the SAT-encoding.
Even if the lexicon gives all readings that exist in the lexicon, the
user might give a nonexistent reading, or in the case of MAP, a
syntactic tag, which is (by definition) not in the lexicon. We may need to move
to a more scalable solution.
% would be to make each tag a variable, and ask the
%question ``can this reading be a noun? how about singular? how about
%conditional?'' separately for each tag. Then we could lift the
%restriction of tag combinations into a SAT solver: make SAT clauses
%that prohibit a comparative to go with a verb, or conditional with a noun.


% \paragraph{Regular expressions}
% For rules with regular expressions, this would blow up even more: there are rules that only address whether the word form in a condition starts with a lowercase letter, or whether it ends in a certain suffix. A realistic, albeit unsatisfactory option is just to treat it as an underspecified reading, and offer that same regular expression to the user when generating a symbolic sentence. Another, less feasible, solution is to grep all the word forms or lemmas that match the regular expression in question---in the worst case (e.g. lower case), this will match all the entries in the expanded lexicon.

\paragraph{Support for grammar writers}
One additional feature is to suggest reformattings for a rule. Recall
figure~\ref{fig:regroup} from the introduction; in that case, the
original rule was written by the original author, and another
grammarian thought that the latter form is nicer to read. Doing the
reverse operation could also be possible. If a rule with long
disjunctions conflicts, it may be useful to split it into smaller
conditions, and eliminate one at a time, in order to find the
reason(s) for the conflict.
Our next step is to evaluate our tools together with actual grammar writers,
in comparison with a corpus-based method or machine learning.

\paragraph{Other grammar formalisms}
Finally, we would like to investigate logic-based methods for analysing other grammar
formalisms, for example Grammatical Framework \cite{gf}. (INARI: can you add a reference here?)

%\subsection{Conclusions}

%It is nice!

% * Preliminary results
%  - dutch & spanish
%  - mention scalability
%  - talk about size of SAT problem -- give number of SAT clauses for the last rule in the biggest grammar I have

% * Future work
%  - analysing different grammar formalisms
%  - asking different questions
%  - restrict yourself to readings that are actually words
