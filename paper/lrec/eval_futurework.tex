\section{Evaluation}
\label{sec:eval}

We tested three grammars to find conflicting rules: 
Dutch\footnote{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-nld/apertium-nld.nld.rlx}},
with 59 rules; 
Spanish\footnote{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-spa/apertium-spa.spa.rlx}},
with 279 rules; and 
Finnish\footnote{\url{https://github.com/flammie/omorfi/blob/master/src/vislcg3/omorfi.cg3}},
with 1185 rules. We left out \textsc{add}, \textsc{map} and other rule
types introduced in CG-3, and only tested \textsc{remove} and \textsc{select} rules.
The results are shown in table~\ref{table:res}. 


\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}

\hline
              & \textsc{nld}  & \textsc{spa}  & \textsc{fin}  \\ \hline
\# rules      & 59              & 279      & 1185     \\ \hline
\# readings   & 336             & 2191     & 2012    \\ \hline
\# conflicts  & 7 (1 + 5 + 1)   & 1        & 51    \\ \hline
\clock{} all rules       & ??s              & ??min ??s    & ?h ??min    \\ \hline
\clock{} last rule       & ??s              & ??s          & ?min ??s    \\ \hline

%\# variables: last rule & 33915         & 247153            &    \\ \hline

\end{tabular}
\caption{\todo{make more tables}}
\label{table:res}
\end{table}

\paragraph{Accuracy} 
The experiments revealed problems in all grammars. For the smaller
grammars, we were able to verify manually that the detected rules were
true positives.
%The results for the Finnish grammar are inconclusive, 
% For the Finnish grammar, we did not have time to
% investigate all of them, but among those we could, we found both true and false negatives.
We did not check for false negatives in any of the grammars. However, the test reported a high number of errors even in the smallest grammars, and some of them are intuitively not that bad; we 


\paragraph{Dutch} The Dutch grammar had two kinds of errors: rule-internal, and rule interaction. As for rule-internal conflicts, one was due to a misspelling in the list definition for personal pronouns, which rendered 5 rules ineffective. The other was about subreadings: the genitive \emph{s} is analysed as a subreading in the Apertium morphological analyser, but it appeared in 2 rules as the main reading. 

There was one genuine conflict with rule interaction, shown below:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE Adv IF (1 Noun) ;
REMOVE Adv IF (-1 Det) (0 Adj) (1 Noun) ;
\end{verbatim}
\end{itemize}

Two rules, which both remove an adverb, were in an order where the first has more broad condition: remove adverb if followed by a noun. In contrast, the second rule has a stricter requirement: only remove adverb if it is preceded by a determiner; the adverb itself is ambiguous with an adjective, and followed by a noun. The proble is that the first rule removes the adverb in all possible cases, and the second will not have any chance to act. If the rules were in the opposite order, then there would be no problem.


%%% Nevermind, this wasn't a conflict, my program was just buggy :-P
%This is not a conflict, just pointing out an example how the SAT-solver constructs the only possible sentence where this works.

%\begin{itemize}
%\item[] 
%\begin{verbatim}
%SELECT DetPosNotZijn IF (1 Noun) ;
%SELECT DetPos IF (-1 (vbser pres p3 sg)) 
%                 (0 "zijn") (1 Noun);
%\end{verbatim}
%\end{itemize}

% SELECT DetPosNotZijn IF (1 Noun);
%If the first rule fires, then the condition of the second rule cannot be true: there can never be a ``zijn'' in the target, because the previous rule has selected everything that is a possessive determiner and not ``zijn''. 
%So, the first rule must not fire. It cannot be because condition does not hold: the condition (1 Noun) is shared with the second rule, and  

We also tested rules individually, in a way that a grammar writer might use our tool when writing new rules.
The following rule was one of them:

\begin{itemize}
\item[] 
\texttt{SELECT DetPos IF (-1 (vbser pres p3 sg)) (0 "zijn") (1 Noun);}
\end{itemize} 

As per VISL CG-3, the condition \texttt{(0 "zijn")} does not require that the input would have determiner and ``zijn'' in the same reading: just that there is any determiner and any ``zijn'' in the cohort. 

\todo{Find a better example of the imprecise 0 condition --- the ambiguity class thing would eliminate this one!}

Can we catch this imprecise formulation with our tool? The example word constructed by the SAT-solver may as likely be one of the following:

\begin{itemize}
\item[a.] \begin{verbatim}
"w2"
    w2<det><pos><f><sg>
    w2<vbser><inf><"zijn">
\end{verbatim}

\item[b.] \begin{verbatim}
"w2"
    w2<det><pos><mfn><pl><"zijn">
    w2<anything>
\end{verbatim}
\end{itemize}

Incidentally, the first example of a rule conflict had a similar requirement, but used correctly: the intended semantics was indeed that the adjective and the adverb are in different readings. However, in this case, the correct way to express the target is \texttt{SELECT DetPos + "zijn"}.

In the latter case, the grammar writer would hopefully notice the imprecise definition and change their definition. However, this is more of a happy side effect than intended feature---the grammar writer cannot count on our tool to notice all similar cases.
This kind of confusion is found in other rules as well. 
To catch them more systematically, we could add a feature that alerts in all cases where a condition with 0 is used, and automatically construct a version of the rule that moves whatever tags in the 0-condition into the target, then asks the user which one was meant.

\paragraph{Spanish} The Spanish grammar had proportionately the highest number of errors.\footnote{TODO://link/to/error.log} \todo{confirm; I assume this is gonna be so, but let's see Fin and Por}.  The grammar we ran is like the one found in the Apertium repository, apart from 2 changes: we fixed some typos (capital O for 0) in order to make it compile, and commented out two rules that used regular expressions, because our solution does not handle them properly (see more on Future work/Implementation).

Again, we can classify the conflicts into internal and interaction. As an example of internal conflict, there are \todo{n} rules that use \texttt{SET Cog = (np cog)}---the problem is that the tag \texttt{cog} does not exist in the Apertium dictionary for Spanish. It is likely that this grammar has been written for an earlier version, where such tag has been in place.




The Spanish grammar has a high number of interaction conflicts, all with the same pattern:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE (a) IF (-1 x) (1 y) ;
REMOVE (a) IF (-1 x) (1 y) (NOT -1 z);
\end{verbatim}
\end{itemize}


This is another instance of more general rule applying before more specific. Negated context works as well to make a rule stricter. The second rule will never apply, because the first will catch all cases.

In addition, there was a number of definitions that were never used. The program doesn't currently point them out to the user, but that is a trivial addition; just like rule-internal conflicts, it doesn't require any semantic analysis. 
Another error, which was not pointed by the program, but discovered when we tried to extract the readings from the grammar and use them 



As with the Dutch grammar, we ran the tool on individual rules and examined the sequences that were generated. None of the following was marked as a conflict, but looking at the output indicated that 

\begin{itemize}
\item[] 
\begin{verbatim}
REMOVE Vblex IF (0 "como") (*-1 Vblex BARRIER Cnj_Rel) ;
REMOVE Vblex IF (0 "como") (*1 Vblex BARRIER Cnj_Rel) ;
\end{verbatim}
\end{itemize}

Here is something that the ambiguity class constraint should find out.
The grammar writer wants to disambiguate the word form ``como'', which can be either a preposition, adverb or the first person singular of the verb ``comer''. 
However, the rule here addresses the lemma ``como'' instead of the word form, hence the SAT-solver cannot create a solution where \texttt{vblex} and ``como'' are in the same reading. The automated 0-to-target conversion could come in handy here too.

Side note: if this rule was written as \texttt{Vblex + "como"}, it would have been caught as a conflicting tag combination. ``Como'' as a lemma does not occur with \texttt{vblex}; it is only one word form which is ambiguous.

\begin{itemize}
\item[] 
\begin{verbatim}
REMOVE Podar  IF (0 Podar) (1 Adv) (2C Inf OR Enc) ;
REMOVE Sentar IF (0 Sentar) (1C Inf OR Enc) ;
\end{verbatim}
\end{itemize}

The comments make it clear that the first rule is meant to disambiguate between poder and podar, and the second rule between sentir and sentar. But the rule does not mention anything about ``poder'' nor ``sentir''; the SAT-solver gave a triggering sequence without 

This is actually a shortcoming of our way of handling the lemmas. If the lemmas and word forms were integrated to the analyses in a more feasible way, then the ambiguity class constraints would create a sequence where ``podar'' was ambiguous with ``poder'', instead of randomly chosen readings. So, even though this was not marked as a conflict, we can say that the weird output of the tool was unnecessary, and by better implementation, the tool would have created more realistic example sequence.




\paragraph{Finnish} \todo{Rerun the tests and rewrite this whole section!} For the Finnish grammar, we did not have time to investigate all the
results. We could verify approximately \nicefrac{1}{3} as true positives and
\nicefrac{1}{3} as false positives.
The verified true positives were all rule-internal conflicts. 
For example, the following rule requires the target to be genitive and unambiguously nominative: 
\begin{itemize}
\item[]\texttt{"oma" SELECT GEN IF [...] (0C NOM) ;}
\end{itemize}
We also detected a number of tagset errors;
the grammar was originally written in 1995, and converted to the
Omorfi tagset by \cite{pirinen2015}, but there were 15 rules that used the old convention for possessive suffixes.

%  For example, the Omorfi tagset
% marks a possessive suffix with \texttt{Px}, e.g. \texttt{PxPl2} for second person plural, but the
% grammar uses the old convention \texttt{PL2} in the rules.
% As a result, 15 rules with possessive suffixes were detected as
% impossible.
To improve performance, we ignored the fully specified readings from the
lexicon and used only tag combinations from the grammar:
for example, the rule \texttt{REMOVE (verb sg) IF (-1 det)}
gives us ``verb sg'' and ``det''.
The shortcut works most of the time, but it is possible to
miss some cases: e.g. \texttt{SELECT PRON + REL IF (0 NOM)} 
assumes ``pron rel nom'', but our method only gives
``pron rel'' and ``nom'' separately. 
17 false positives were caused by this shortcut. 
In addition, 2 false positives were caused by the naive implementation of
rules with \verb!*! in their context.

Due to these reasons, the results about the Finnish grammar are
inconclusive. We only include them to show some preliminary data on larger grammars.
%For a condition situated $n$ or more words to the left or right, we only create a word exactly $n$ words away. This is something to be fixed in the future.

\paragraph{Performance} The speed of the check is an obvious issue,
especially for the Finnish grammar. 
As mentioned before, we used 2000 readings extracted from the grammar.
With the full set of 6000 readings from the lexicon, the speed
was even worse: checking the last rule took 30 minutes, as opposed to
3 minutes with 2000 readings. 
%We never finished running the whole rule set with 6000 readings, but
%it would have taken potentially days.
We believe we can still signifcantly improve the performance of the SAT-encoding. Currently it is just a naive proof-of-concept implementation, and we have not tried to optimise for speed at all.

% A Finnish symbolic word has around 5700 variables in the beginning of the analysis, each representing a possible tag combination.
% Around 3700 come from the Apertium morphological lexicon, excluding clitics, and around 2000 are defined in the grammar, as a target (\texttt{REMOVE ("bear" verb) IF ...}) or as a condition (\texttt{REMOVE ... IF (-1 ("bear" noun))}. The majority of them are lexical, whereas we don't include anything but morphological tags from the lexicon.
% For Spanish and Dutch, there are around 2000 and 300 tag combinations respectively.
% The Spanish lexicon includes up to two clitics (e.g. \emph{d√°melo} `give.me.it') in the same word form, but Dutch has none in the lexicon.

% The SAT problem for checking one rule grows depending on how many rules come before it, how long the context is, and how many tag combinations are there. The running time of the last rule gives some indication ...


% Not all questions are so expensive.
% For instance, checking the tag sets in the Finnish grammar doesn't require the context of the other rules at all,
%  The majority of the correctly detected errors in the Finnish grammar were rule-internal---these errors don't need the context of the other rules at all.


\section{Future work}


Our solution to hardcode the tag combinations in the readings is feasible for simple morphology, but it can cause problems with more complex morphology. One big downside is that in order to implement \textsc{ADD}, \textsc{ADDREADING} and \textsc{MAP}, we need to be prepared for new readings--even if the lexicon gives all readings that exist in the lexicon, the user might give a nonexistent reading, or in the case of MAP, a syntactic tag, which is (by definition) not in the lexicon. A more scalable solution would be to make each tag a variable, and ask the question ``can this reading be a noun? how about singular? how about conditional?'' separately for each tag. Then we could lift the restriction of tag combinations into a SAT solver: make SAT clauses that prohibit a comparative to go with a verb, or conditional with a noun.
Another benefit of this solution is the addition of lemmas and word forms, as well as regular expressions: currently, if we want to add one more lemma for a verb, we need to create as many new variables as there are distinct verb forms---easily hundreds for languages with rich morphology.
\todo{less rambling: For rules with regular expressions, this would blow up even more: there are rules that only address whether the word form in a condition starts with a lowercase letter, or whether it ends in a certain suffix. A realistic, albeit unsatisfactory option is just to treat it as an underspecified reading, and offer that same regular expression to the user when generating a symbolic sentence. Another, less feasible, solution is to grep all the word forms or lemmas that match the regular expression in question---in the worst case (e.g. lower case), this will match all the entries in the expanded lexicon.}


As for longer-term goals, we want to handle the full expressivity of CG-3,
with \textsc{map}, \textsc{add} and \textsc{substitute} rules, and
dependency structure. This also means finding different kinds of conflicts.
As soon as the tools are mature enough, we want to
evaluate them with actual grammar writers,
in comparison with a corpus-based method or machine learning.
Finally, if the method proves feasible for CG, we want
to try applying it to other grammar formalisms.


% * Preliminary results
%  - dutch & spanish
%  - mention scalability
%  - talk about size of SAT problem -- give number of SAT clauses for the last rule in the biggest grammar I have

% * Future work
%  - analysing different grammar formalisms
%  - asking different questions
%  - restrict yourself to readings that are actually words
