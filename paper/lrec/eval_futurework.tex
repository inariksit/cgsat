\section{Evaluation and Future Work}
\label{sec:eval}

We tested three grammars to find conflicting rules: 
Dutch\footnote{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-nld/apertium-nld.nld.rlx}},
with 59 rules; 
Spanish\footnote{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-spa/apertium-spa.spa.rlx}},
with 279 rules; and 
Finnish\footnote{\url{https://github.com/flammie/omorfi/blob/master/src/vislcg3/omorfi.cg3}},
with 1185 rules. We left out \textsc{add}, \textsc{map} and other rule
types introduced in CG-3, and only tested \textsc{remove} and \textsc{select} rules.
The results are shown in table~\ref{table:res}. 


\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
                      & \textsc{nld}  & \textsc{spa}  & \textsc{fin}  \\ \hline
\# rules      & 59              & 279      & 1185     \\ \hline
\# readings      & 276             & 2191     & 2012    \\ \hline
\# conflicts       & 1               & 1        & 51    \\ \hline
\clock{} all rules       & 7s              & 3min 22s    & 3h 26min    \\ \hline
\clock{} last rule       & 4s              & 7s                & 3min 40s    \\ \hline
% This is with just 2000ish variables for Finnish, below is with 5700

% \clock{} last rule       & 4s              & 7s          & 30min 25s    \\ \hline
% \clock{} longest rule    & ?s              & ?s                & ?min     \\ \hline
%\# variables: last rule & 33915         & 247153            &    \\ \hline

\end{tabular}
\caption{Detecting rules that cannot be applied}
\label{table:res}
\end{table}

\paragraph{Accuracy} 
The experiments revealed problems in all grammars. For the smaller
grammars, we were able to verify manually that the detected rules were
true positives.
%The results for the Finnish grammar are inconclusive, 
% For the Finnish grammar, we did not have time to
% investigate all of them, but among those we could, we found both true and false negatives.
We did not check for false negatives in any of the grammars.

The Spanish grammar had an erroneous set definition, where a word was
required to have almost all POS tags at the same time. After fixing
the set definition, our test did not reveal other problems.
The Dutch grammar had the following rules in the given order, making the latter impossible to apply:
\begin{itemize}
\item[] 
\begin{verbatim}REMOVE Adv IF (1 Noun) ;
REMOVE Adv IF (-1 Det) (0 Adj) (1 Noun) ;
\end{verbatim}
\end{itemize}

For the Finnish grammar, we did not have time to investigate all the
results. We could verify approximately \nicefrac{1}{3} as true positives and
\nicefrac{1}{3} as false positives.
The verified true positives were all rule-internal conflicts. 
For example, the following rule requires the target to be genitive and unambiguously nominative: 
\begin{itemize}
\item[]\texttt{"oma" SELECT GEN IF [...] (0C NOM) ;}
\end{itemize}
We also detected a number of tagset errors;
the grammar was originally written in 1995, and converted to the
Omorfi tagset by \cite{pirinen2015}, but there were 15 rules that used the old convention for possessive suffixes.

%  For example, the Omorfi tagset
% marks a possessive suffix with \texttt{Px}, e.g. \texttt{PxPl2} for second person plural, but the
% grammar uses the old convention \texttt{PL2} in the rules.
% As a result, 15 rules with possessive suffixes were detected as
% impossible.
To improve performance, we ignored the fully specified readings from the
lexicon and used only tag combinations from the grammar:
for example, the rule \texttt{REMOVE (verb sg) IF (-1 det)}
gives us ``verb sg'' and ``det''.
The shortcut works most of the time, but it is possible to
miss some cases: e.g. \texttt{SELECT PRON + REL IF (0 NOM)} 
assumes ``pron rel nom'', but our method only gives
``pron rel'' and ``nom'' separately. 
17 false positives were caused by this shortcut. 
In addition, 2 false positives were caused by the naive implementation of
rules with \verb!*! in their context.

Due to these reasons, the results about the Finnish grammar are
inconclusive. We only include them to show some preliminary data on larger grammars.
%For a condition situated $n$ or more words to the left or right, we only create a word exactly $n$ words away. This is something to be fixed in the future.

\paragraph{Performance} The speed of the check is an obvious issue,
especially for the Finnish grammar. 
As mentioned before, we used 2000 readings extracted from the grammar.
With the full set of 6000 readings from the lexicon, the speed
was even worse: checking the last rule took 30 minutes, as opposed to
3 minutes with 2000 readings. 
%We never finished running the whole rule set with 6000 readings, but
%it would have taken potentially days.
We believe we can still signifcantly improve the performance of the SAT-encoding. Currently it is just a naive proof-of-concept implementation, and we have not tried to optimise for speed at all.

% A Finnish symbolic word has around 5700 variables in the beginning of the analysis, each representing a possible tag combination.
% Around 3700 come from the Apertium morphological lexicon, excluding clitics, and around 2000 are defined in the grammar, as a target (\texttt{REMOVE ("bear" verb) IF ...}) or as a condition (\texttt{REMOVE ... IF (-1 ("bear" noun))}. The majority of them are lexical, whereas we don't include anything but morphological tags from the lexicon.
% For Spanish and Dutch, there are around 2000 and 300 tag combinations respectively.
% The Spanish lexicon includes up to two clitics (e.g. \emph{d√°melo} `give.me.it') in the same word form, but Dutch has none in the lexicon.

% The SAT problem for checking one rule grows depending on how many rules come before it, how long the context is, and how many tag combinations are there. The running time of the last rule gives some indication ...


% Not all questions are so expensive.
% For instance, checking the tag sets in the Finnish grammar doesn't require the context of the other rules at all,
%  The majority of the correctly detected errors in the Finnish grammar were rule-internal---these errors don't need the context of the other rules at all.


\paragraph{Future work} 

We will work on the performance, and fix the errors we found in the
evaluation, such as the issue with non-exact position.
We also plan to restrict possible ambiguities in the symbolic words. 
Currently we have only hard-coded some
very basic assumptions, such as ``word cannot be ambiguous between a
boundary and a real word'', but we want to extract these constraints
automatically from a lexicon.
% For instance, the Spanish lexicon contains the entries
% \texttt{casa:casa<n><sg>}  and  \texttt{casa:casar<v><sg><p3>}, thus
% we know that a word can be ambiguous between a noun and a verb.
By adding these restrictions to the symbolic words, we hope to
restrict the search space, and make the SAT problem easier to solve, while at the same time generate more realistic example sentences.

As for longer-term goals, we want to handle the full expressivity of CG-3,
with \textsc{map}, \textsc{add} and \textsc{substitute} rules, and
dependency structure. This also means finding different kinds of conflicts.
As soon as the tools are mature enough, we want to
evaluate them with actual grammar writers,
in comparison with a corpus-based method or machine learning.
Finally, if the method proves feasible for CG, we want
to try applying it to other grammar formalisms.


% As another line of investigation, we want to detect more kinds of conflicts.
% For instance, say we have the following rules:

% \begin{itemize}
% \item[] \begin{verbatim}
% REMOVE verb IF (-1 det) ;
% REMOVE noun IF (-1 det) ;
% \end{verbatim}
% \end{itemize}

% With our symbolic sentence, these rules will be no problem; to apply the latter, we only need to construct a target that has a realistic ambiguity with a noun; the verb is gone already.
% However, in the context of real life texts, the grammar writer might actually mean the following:

% \begin{itemize}
% \item[] \begin{verbatim}
% REMOVE verb IF (-1 det) (0 noun) ;
% REMOVE noun IF (-1 det) (0 verb) ;
% \end{verbatim}
% \end{itemize}

% In that case, those rules are contradictory, and we would like to find it out.



% * Preliminary results
%  - dutch & spanish
%  - mention scalability
%  - talk about size of SAT problem -- give number of SAT clauses for the last rule in the biggest grammar I have

% * Future work
%  - analysing different grammar formalisms
%  - asking different questions
%  - restrict yourself to readings that are actually words
