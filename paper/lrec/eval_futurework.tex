\section{Evaluation}
\label{sec:eval}

We tested three grammars to find conflicting rules: 
Dutch\footnote{\scriptsize{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-nld/apertium-nld.nld.rlx}}},
with 59 rules; 
Spanish\footnote{\scriptsize{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-spa/apertium-spa.spa.rlx}}},
with 279 rules; and 
Finnish\footnote{\scriptsize{\url{https://github.com/flammie/apertium-fin/raw/master/apertium-fin.fin.rlx}}},
with 1185 rules. We left out \textsc{add}, \textsc{map} and other rule
types introduced in CG-3, and only tested \textsc{remove} and \textsc{select} rules.
The results for Dutch and Spanish are shown in table~\ref{table:res},
and the results for Finnish in table~\ref{table:resFin}.

A natural follow-up evaluation would be to compare the performance of the
grammar in the original state, and after removing the conflicts found
by our tool. Unfortunately, we did not have time to perform such
evaluation, and in addition, we only have gold standard corpus (20~000~words) for Spanish.



\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}

\hline
                   & \textsc{nld}  & \textsc{spa}lem  & \textsc{spa}nolem \\ \hline
\# rules           & 59            & 279       & 279     \\ \hline
\# readings        & 336           & 3905      & 1735    \\ \hline
\# true positives  & 7             & n         & m    \\ \hline
\# false positives & 0             & n         & m    \\ \hline
\clock{} with amb. 
           classes & ??s           & + 1h      & 30m-1h   \\ \hline
% \clock{} last rule & ??s           & ??s       & ?min ??s    \\ \hline
\clock{} no amb. 
           classes & 10ish sec       & ??min ??s    & ?h ??min    \\ \hline

%\# variables: last rule & 33915         & 247153            &    \\ \hline

\end{tabular}
\caption{Results for Dutch and Spanish grammars.}
\label{table:res}
\end{table}




The experiments revealed problems in all grammars. For the smaller
grammars, we were able to verify manually that the detected rules were
true positives.
%The results for the Finnish grammar are inconclusive, 
% For the Finnish grammar, we did not have time to
% investigate all of them, but among those we could, we found both true and false negatives.
We did not systematically check for false negatives in any of the
grammars, but we kept track of a number of known tricky cases; mostly
rules with negations and complex set operations.
%While this does not guarantee the lack of false negatives, it indicates some degree of reliability. 
As the tool matures and we add new features, a more in-depth analysis
will be needed.

\subsection{Dutch} The Dutch grammar had two kinds of errors: rule-internal, and rule interaction. As for rule-internal conflicts, one was due to a misspelling in the list definition for personal pronouns, which rendered 5 rules ineffective. The other was about subreadings: the genitive \emph{s} is analysed as a subreading in the Apertium morphological analyser, but it appeared in 2 rules as the main reading. 

There was one genuine conflict with rule interaction, shown below:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE Adv IF (1 N) ;
REMOVE Adv IF (-1 Det) (0 Adj) (1 N) ;
\end{verbatim}
\end{itemize}

Two rules, which both remove an adverb, were in an order where the first has more broad condition: remove adverb if followed by a noun. In contrast, the second rule has a stricter requirement: only remove adverb if it is preceded by a determiner; the adverb itself is ambiguous with an adjective, and followed by a noun. The proble is that the first rule removes the adverb in all possible cases, and the second will not have any chance to act. If the rules were in the opposite order, then there would be no problem.


%%% Nevermind, this wasn't a conflict, my program was just buggy :-P

%\begin{itemize}
%\item[] 
%\begin{verbatim}
%SELECT DetPosNotZijn IF (1 Noun) ;
%SELECT DetPos IF (-1 (vbser pres p3 sg)) 
%                 (0 "zijn") (1 Noun);
%\end{verbatim}
%\end{itemize}

% SELECT DetPosNotZijn IF (1 Noun);
%If the first rule fires, then the condition of the second rule cannot be true: there can never be a ``zijn'' in the target, because the previous rule has selected everything that is a possessive determiner and not ``zijn''. 
%So, the first rule must not fire. It cannot be because condition does not hold: the condition (1 Noun) is shared with the second rule, and  

We also tested rules individually, in a way that a grammar writer might use our tool when writing new rules.
The following rule was one of them:

\begin{itemize}
\item[] 
\texttt{SELECT DetPos IF (-1 (vbser pres p3 sg)) (0 "zijn") (1 Noun);}
\end{itemize} 

As per VISL CG-3, the condition \texttt{(0 "zijn")} does not require
that the input would have determiner and ``zijn'' in the same reading:
just that there is a reading with any determiner, and a reading with any ``zijn'' in the cohort. 
Can we catch this imprecise formulation with our tool? The example word constructed by the SAT-solver may as likely be one of the following:

\begin{itemize}
\item[a.] \begin{verbatim}
"w2"
    w2<det><pos><f><sg>
    w2<vbser><inf><"zijn">
\end{verbatim}

\item[b.] \begin{verbatim}
"w2"
    w2<det><pos><mfn><pl><"zijn">
\end{verbatim}
\end{itemize}

Incidentally, the first example of a rule conflict had a similar
requirement, but used correctly: the intended semantics was indeed
that the adjective and the adverb are in different readings. However,
in this case, the precise way to express the target is \texttt{SELECT DetPos + "zijn"}.

We found the same kind of definitions in many other rules and grammars.
To catch them more systematically, we could add a feature that alerts in all cases where a condition with 0 is used, and automatically construct a version of the rule that moves whatever tags in the 0-condition into the target, then asks the user which one was meant.

% In the latter case, the grammar writer would hopefully notice the imprecise definition and change their definition. However, this is more of a happy side effect than intended feature---the grammar writer cannot count on our tool to notice all similar cases.

\subsection{Spanish} The Spanish grammar had proportionately the
highest number of errors. The grammar we ran is like the one
found in the Apertium repository, apart from two changes: we fixed
some typos (capital O for 0) in order to make it compile, and
commented out two rules that used regular expressions, because our
solution does not handle them properly; see more on Section~\ref{sec:conclusion}.

Again, we can classify the conflicts into internal and interaction. As an example of internal conflict, there are \todo{n} rules that use \texttt{SET Cog = (np cog)}---the problem is that the tag \texttt{cog} does not exist in the Apertium dictionary for Spanish. It is likely that this grammar has been written for an earlier version, where such tag has been in place.


The Spanish grammar has a high number of interaction conflicts, \todo{n} with the same pattern:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE (a) IF (-1 x) (1 y) ;
REMOVE (a) IF (-1 x) (1 y) (NOT -1 z);
\end{verbatim}
\end{itemize}


This is another instance of more general rule applying before more
specific. Negated context works as well to make a rule stricter. The
second rule will never apply, because the first will catch all
cases. For a full list of found conflicts, see \todo{\url{actual://link.here/spanish/error.log}}

In addition, there was a number of set definitions that were never
used. Since VISL CG-3 already points out unused sets, we did not add such
feature in our tool. However, we noticed an unexpected benefit when
we tried to use the set definitions from the grammar directly as our
readings: this way, we can discover inconsistencies even in
set definitions that are not used in any rule.
%If a set definition is impossible, using it in a rule makes the rule have an internal conflict.
For instance, the following definition requires the word to be all of
the listed parts of speech at the same time---the grammar writer meant
OR instead of +:
\begin{itemize}
\item[] 
\texttt{SET NP\_Member = N + A + Det + PreAdv + Adv + Pron ;}
\end{itemize}

If it was used in any rule, that rule would have been marked as
conflicting. We noticed the error by accident, when the program
offered the reading \texttt{w2<n><np><adj><det><preadv><adv><prn>}
in an example sequence meant for another rule.


As with the Dutch grammar, we ran the tool on individual rules and
examined the sequences that were generated. None of the following was
marked as a conflict, but looking at the output indicated that there
are multiple interpretations, such as whether two analyses for a
context word should be in the same reading or different readings.
We observed also cases where the grammar writer has specified desired
behaviour in comments, but the rule does not do what the grammar
writer intended. 

% \begin{itemize}
% \item[] 
% \begin{verbatim}
% REMOVE Vblex IF (0 "como") (*-1 Vblex BARRIER Cnj_Rel) ;
% \end{verbatim}
% \end{itemize}

% Based on the comments in the file, the grammar writer wants to
% disambiguate the word form ``como'', which can be either a
% preposition, adverb or the first person singular of the verb
% ``comer''. 
% However, the rule here addresses the \emph{lemma} ``como'' instead of the
% word form, hence the SAT-solver cannot create a solution where
% \texttt{vblex} and ``como'' as a lemma are in the same
% reading. 
% A precise definition for this rule would target \texttt{vblex "<como>"} 
% or \texttt{vblex "comer"}, so that it becomes obvious that those two
% are supposed to be in the same reading. 

% Currently, as all the word forms are allowed to combine with anything,
% the ambiguity class constraint does not point this
% The automated 0-to-target conversion could come in handy here too.
% However, if the 

% TODO An example of the same but in condition. that the ambiguity class constraint, when
% implemented properly, should find out.



\begin{itemize}
\item[] 
\texttt{REMOVE Podar  IF (0 Podar) (1 Adv) (2C Inf OR Enc) ;} \\
\texttt{REMOVE Sentar IF (0 Sentar) (1C Inf OR Enc) ;}
\end{itemize}

The comments make it clear that the first rule is meant to disambiguate between poder and podar, and the second rule between sentir and sentar. But the rule does not mention anything about ``poder'' nor ``sentir''; the SAT-solver gave a triggering sequence without 
We found the same kind of formulation in many other rules and grammars.
To catch them more systematically, we could add a feature that alerts in all cases where a condition with 0 is used, and automatically construct a version of the rule that moves whatever tags in the 0-condition into the target, then asks the user which one was meant.

This example shows a shortcoming of our way of handling the lemmas. If
the lemmas and word forms were integrated to the analyses in a more
feasible way, then the ambiguity class constraints would create a
sequence where ``podar'' was ambiguous with ``poder'', only in the
word worm ``<puedo>'' instead of randomly chosen readings. So, even though this was not marked as a conflict, we can say that the weird output of the tool was unnecessary, and by better implementation, the tool would have created more realistic example sequence.


\begin{table*}[t!]
\centering
\begin{tabular}{|l|l|l|l|}

\hline
              & 1 clitic & 2 clitics & readings only from the grammar \\ \hline
\# readings   & many     & more & lagom   \\ \hline
\# true 
   positives  & n   & m        & o    \\ \hline
\# false 
   positives  & p   & q        & super many    \\ \hline
\clock{}amb.
      classes & ??s              & ??min ??s    & ?h ??min  \\ \hline
\clock{}no amb.
      classes & ??s              & ??s          & ?min ??s  \\ \hline


\end{tabular}
\caption{Results for Finnish (1185 rules).}
\label{table:resFin}
\end{table*}

\subsection{Finnish} 

The results for the Finnish grammar are shown separately, in table~\ref{table:resFin}.
We were not able to use the method with ambiguity classes at all---expanding the Finnish morphological lexicon results in \todo{100s of gigabytes?} of word forms, which is simply too big for our method to work. 
For future development, we will see if it is possible to manipulate the finite automata directly, instead of relying on the output in text.

In addition to the previous problem, we had a large number of
readings. In order to make the test run faster, we used a few shortcuts.
shortcuts. 
Firstly, we restricted the amount of clitics per word form, and even
ignored all the specified readings.
Secondly, we added ambiguity class constraints for only
\emph{intraparadigmatic ambiguities}: homonymous wordforms belonging
to the same lexeme, in a way that is systematic to the whole
paradigm. We reduced the morphological lexicon to contain only one
example of each inflection paradigm, and then expanded it: with all
the clitics, this resulted in \todo{n} word forms. Then we applied the
normal method of creating the ambiguity classes, and specified some
default cases manually, such as punctuation and word boundaries.
Thirdly, we ignored the fully specified readings from the
lexicon and used only tag combinations from the grammar, as described
in Section~\ref{sec:realistic_readings}.

The results of these three variants are described in
table~\ref{table:resFin}. As expected, using only readings from the
grammar resulted in most false positives. 


\subsection{Performance} 
The running time of the grammars goes from seconds to hours. 
As can be seen from the times, increasing the number of readings
matters more than increasing the number of rules.
In addition, the ambiguity class constraints take a long time. 

However, many of the use cases do not require running the whole
grammar. Testing the interaction between 5--10 rules takes just
seconds, regardless of the language. 

%We believe we can still signifcantly improve the performance of the SAT-encoding. Currently it is just a naive proof-of-concept implementation, and we have not tried to optimise for speed at all.

% A Finnish symbolic word has around 5700 variables in the beginning of the analysis, each representing a possible tag combination.
% Around 3700 come from the Apertium morphological lexicon, excluding clitics, and around 2000 are defined in the grammar, as a target (\texttt{REMOVE ("bear" verb) IF ...}) or as a condition (\texttt{REMOVE ... IF (-1 ("bear" noun))}. The majority of them are lexical, whereas we don't include anything but morphological tags from the lexicon.
% For Spanish and Dutch, there are around 2000 and 300 tag combinations respectively.
% The Spanish lexicon includes up to two clitics (e.g. \emph{dámelo} `give.me.it') in the same word form, but Dutch has none in the lexicon.

% The SAT problem for checking one rule grows depending on how many rules come before it, how long the context is, and how many tag combinations are there. The running time of the last rule gives some indication ...


% Not all questions are so expensive.
% For instance, checking the tag sets in the Finnish grammar doesn't require the context of the other rules at all,
%  The majority of the correctly detected errors in the Finnish grammar were rule-internal---these errors don't need the context of the other rules at all.


\section{Future work and conclusions}
\label{sec:conclusion}

The evaluation indicates that the tool finds conflicts and dead rules
from actual grammars. We did not have a volunteer to test the tool in
the process of grammar writing, so we cannot conclude whether the
constructed examples are useful for getting new insights on the rules.
However, there are still a number of features to improve and add.

\paragraph{Combining morphological and lexical tags}

Our solution to hardcode the tag combinations in the readings is
feasible for simple morphology, but it can cause problems with more
complex morphology.
Another benefit of this solution is the addition of lemmas and word
forms, as well as regular expressions: currently, if we want to add
one more lemma for a verb, we need to create as many new variables as
there are distinct verb forms---easily hundreds for languages with
rich morphology.
This may also be feasible to support regular expressions, in a nicer
way than just spitting out the regex. (MORE ON THAT)

\paragraph{Full expressivity of CG-3}
As for longer-term goals, we want to handle the full expressivity of CG-3,
with \textsc{map}, \textsc{add},  \textsc{addreading} and
\textsc{substitute} rules, as well as dependency structure. 
This also means finding different kinds of conflicts.
In order to implement rules that may add new readings, or new tags to
existing readings, we need to modify our approach in the SAT-encoding.
Even if the lexicon gives all readings that exist in the lexicon, the
user might give a nonexistent reading, or in the case of MAP, a
syntactic tag, which is (by definition) not in the lexicon. A more
scalable solution would be to make each tag a variable, and ask the
question ``can this reading be a noun? how about singular? how about
conditional?'' separately for each tag. Then we could lift the
restriction of tag combinations into a SAT solver: make SAT clauses
that prohibit a comparative to go with a verb, or conditional with a noun.


% \paragraph{Regular expressions}
% For rules with regular expressions, this would blow up even more: there are rules that only address whether the word form in a condition starts with a lowercase letter, or whether it ends in a certain suffix. A realistic, albeit unsatisfactory option is just to treat it as an underspecified reading, and offer that same regular expression to the user when generating a symbolic sentence. Another, less feasible, solution is to grep all the word forms or lemmas that match the regular expression in question---in the worst case (e.g. lower case), this will match all the entries in the expanded lexicon.

\paragraph{Support for grammar writers}
One additional feature is to suggest reformattings for a rule. Recall
figure~\ref{fig:regroup} from the introduction; in that case, the
original rule was written by the original author, and another
grammarian thought that the latter form is nicer to read. Doing the
reverse operation could also be possible. If a rule with long
disjunctions conflicts, it may be useful to split it into smaller
conditions, and eliminate one at a time, in order to find the
reason(s) for the conflict.
As soon as the tools are mature enough, we want to
evaluate them with actual grammar writers,
in comparison with a corpus-based method or machine learning.
Finally, if the method proves feasible for CG, we want
to try applying it to other grammar formalisms.

\subsection{Conclusions}

It is nice!

% * Preliminary results
%  - dutch & spanish
%  - mention scalability
%  - talk about size of SAT problem -- give number of SAT clauses for the last rule in the biggest grammar I have

% * Future work
%  - analysing different grammar formalisms
%  - asking different questions
%  - restrict yourself to readings that are actually words
