\section{Evaluation}
\label{sec:eval}

We tested three grammars to find conflicting rules: 
Dutch\footnote{\scriptsize{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-nld/apertium-nld.nld.rlx}}},
with 59 rules; 
Spanish\footnote{\scriptsize{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-spa/apertium-spa.spa.rlx}}},
with 279 rules; and 
Finnish\footnote{\scriptsize{\url{https://github.com/flammie/apertium-fin/raw/master/apertium-fin.fin.rlx}}},
with 1185 rules. We left out \textsc{add}, \textsc{map} and other rule
types introduced in CG-3, and only tested \textsc{remove} and \textsc{select} rules.
The results for Dutch and Spanish are shown in Table~\ref{table:res},
and the results for Finnish in Table~\ref{table:resFin}.

A natural follow-up evaluation would be to compare the performance of the
grammar in the original state, and after removing the conflicts found
by our tool. Unfortunately, we did not have time to perform such
evaluation, and in addition, we only have gold standard corpus (20~000~words) for Spanish.



\begin{table}[]
\centering
\begin{tabular}{|p{2.84cm}|p{1cm}|p{1.15cm}|p{1.55cm}|}
%\begin{tabular}{|p{2.9cm}|p{0.95cm}|p{1.15cm}|p{1.55cm}|}

\hline
                   & \textsc{nld}  & \textsc{spa}  & \textsc{spa}~\small{sep.~lem.} \\ \hline
\# rules           & 59            & 279       & 279     \\ \hline
\# readings        & 336           & 3905      & 1735    \\ \hline
\# conflicts found 
 with amb. classes & 7             & 45        & X      \\ 
{\small (internal~+~interaction)}
                   & {\small
                      (6~+~1)}     & {\small 
                                    (21~+~24)} & {\small (X~+~24)} \\ \hline

\# conflicts found 
 no amb. classes   & 7             & X        & 43      \\ 
{\small (internal~+~interaction)}
                   & {\small
                      (6~+~1)}     & {\small 
                                    (18~+~25)} & {\small (18~+~25)} \\ \hline
% \# false positives 
%  \small{(internal + interaction)}
%                    & 0             & 1 (1 + 0)       & 1 (1 + 0)  \\ \hline

\clock{} with amb. 
           classes & 7 s        & 1h 46m    &  23 min  \\ \hline

\clock{} no amb. 
           classes & 3 s        & 44 min       & 16 min     \\ \hline 


\end{tabular}
\caption{Results for Dutch and Spanish grammars.}
\label{table:res}
\end{table}




The experiments revealed problems in all grammars. For the smaller
grammars, we were able to verify manually that the detected rules were
true positives, save for one case, where we took a shortcut in the Spanish grammar.
%The results for the Finnish grammar are inconclusive, 
% For the Finnish grammar, we did not have time to
% investigate all of them, but among those we could, we found both true and false negatives.
We did not systematically check for false negatives in any of the
grammars, but we kept track of a number of known tricky cases; mostly
rules with negations and complex set operations.

%While this does not guarantee the lack of false negatives, it indicates some degree of reliability. 
As the tool matures and we add new features, a more in-depth analysis
will be needed.

\subsection{Dutch} The Dutch grammar had two kinds of errors: rule-internal, and rule interaction. As for rule-internal conflicts, one was due to a misspelling in the list definition for personal pronouns, which rendered 5 rules ineffective. The other was about subreadings: the genitive \emph{s} is analysed as a subreading in the Apertium morphological analyser, but it appeared in one rule as the main reading. 

There was one genuine conflict with rule interaction, shown below:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE Adv IF (1 N) ;
REMOVE Adv IF (-1 Det) (0 Adj) (1 N) ;
\end{verbatim}
\end{itemize}

% These two rules both remove an adverb, but the first one has a broader condition:
% remove adverb if followed by a noun. In contrast, the second rule has a stricter condition: only remove adverb if it is preceded by a determiner, the adverb itself is ambiguous with an adjective, and followed by a noun. 
The problem is that the first rule has a broader condition than the second.
The first rule removes the adverb in all possible cases where the second would, and the second will not have any chance to act. 
If the rules were in the opposite order, then there would be no problem.



We also tested rules individually, in a way that a grammar writer might use our tool when writing new rules.
The following rule was one of them:

\begin{itemize}
\item[] 
\texttt{SELECT DetPos IF (-1 (vbser pres p3 sg)) (0 "zijn") (1 Noun);}
\end{itemize} 

As per VISL CG-3, the condition \texttt{(0 "zijn")} does not require
that the input would have determiner and \emph{zijn} in the same reading:
just that there is a reading with any determiner, and a reading with any \emph{zijn} in the cohort. 
Can we catch this imprecise formulation with our tool? The SAT-solver will not mark it as a conflict (which is the correct behaviour). But if we ask it to generate an example sequence, the target word may be any of the following options:

\begin{itemize}
\item[a.] \begin{verbatim}
"w2"
    w2<det><pos><f><sg>
    zijn<vbser><inf>
\end{verbatim}

\item[b.] \begin{verbatim}
"w2"
    zijn<det><pos><mfn><pl>
\end{verbatim}
\end{itemize}

Incidentally, the first example of a rule conflict had a similar
requirement, but used correctly: the intended semantics was indeed
that the adjective and the adverb are in different readings. However,
in this case, the precise way to express the target is \texttt{SELECT DetPos + "zijn"}.

We found the same kind of definitions in many other rules and grammars.
To catch them more systematically, we could add a feature that alerts in all cases where a condition with 0 is used. As a possible extension, we could automatically merge the 0-condition into the target reading, then show the user this new version, along with the original, and ask which one was intended.

% In the latter case, the grammar writer would hopefully notice the imprecise definition and change their definition. However, this is more of a happy side effect than intended feature---the grammar writer cannot count on our tool to notice all similar cases.

\subsection{Spanish} The Spanish grammar had proportionately the
highest number of errors. The grammar we ran is like the one
found in the Apertium repository (linked on the previous page), 
apart from two changes: we fixed some typos (capital O for 0) in order to make it compile, and
commented out two rules that used regular expressions, because we did not implement the support for them yet.

We include two versions of the Spanish grammar in Table~\ref{table:res}: in column \textsc{spa}, we added the lemmas and word forms as described in Section~\ref{sec:realistic_readings}, and in column \textsc{spa}~{\small sep.~lem.}, we just added each word form and lemma as individual readings, allowed to combine with any other reading. 
This latter version ran much faster, but failed to detect an internal conflict for one rule, and reported a false positive for another. The numbers look the same
% FAIL TO DETECT: SELECT:pr_cnjadv_3 cnjadv IF (0 "después de") 
% FALSE POSITIVE: SELECT:este_1 det IF (0 "este" + (m sg)|(m sp)|(mf sg)|(mf sp))

Interestingly, the version $with$ ambiguity classes fails to detect a true interaction conflict, because it---correctly---found an interaction conflict in one of the rules.

As an example of internal conflict, there are two rules that use \texttt{SET Cog = (np cog)}---the problem is that the tag \texttt{cog} does not exist in the Apertium dictionary for Spanish. As another example, four rules require a context word tagged as NP with explicit number, but the lexicon does not indicate any number with NPs. It is likely that this grammar has been written for an earlier version, where such tags have been in place.

As for the 25 interaction conflicts, there were only 9 distinct rules that rendered 25 other rules ineffective.
In fact, we can reduce these 9 rules further into 3 different groups: 4 + 4 + 1, where the groups of 4 rules are variants of otherwise identical rule, each with different gender and number.
An example of such conflict is below (gender and number omitted for readability):

\begin{itemize}
\item[] 
\begin{verbatim}
# NOM ADJ ADJ
SELECT A OR PP IF (-2 N) 
 (-1 Adj_PP) (0 Adj_PP) (NOT 0 Det);

# NOM ADJ ADJ ADJ
SELECT A OR PP IF (-3 N) (-2 N) 
 (-1 Adj_PP) (0 Adj_PP) (NOT 0 Det);
\end{verbatim}
\end{itemize}


For a full list of found conflicts, see the log\footnote{\url{https://github.com/inariksit/cgsat/raw/master/data/spa/conflicts.log}} of running our program. 
In addition, the log contains a list of conflicts which were only found by one of the variants.

In addition, the grammar contains a number of set definitions that were never
used. Since VISL CG-3 already points out unused sets, we did not add such
feature in our tool. However, we noticed an unexpected benefit when
we tried to use the set definitions from the grammar directly as our
readings: this way, we can discover inconsistencies even in
set definitions that are not used in any rule.
%If a set definition is impossible, using it in a rule makes the rule have an internal conflict.
For instance, the following definition requires the word to be all of
the listed parts of speech at the same time---most likely, the grammar writer meant 
OR instead of +:
\begin{itemize}
\item[] 
\texttt{SET NP\_Member = N + A + Det + PreAdv + Adv + Pron ;}
\end{itemize}

If it was used in any rule, that rule would have been marked as
conflicting. We noticed the error by accident, when the program
offered the reading \texttt{w2<n><adj><det><preadv><adv><prn>}
in an example sequence meant for another rule.


As with the Dutch grammar, we ran the tool on individual rules and
examined the sequences that were generated. None of the following was
marked as a conflict, but looking at the output indicated that there
are multiple interpretations, such as whether two analyses for a
context word should be in the same reading or different readings.
We observed also cases where the grammar writer has specified desired
behaviour in comments, but the rule does not do what the grammar
writer intended. 

% \begin{itemize}
% \item[] 
% \begin{verbatim}
% REMOVE Vblex IF (0 "como") (*-1 Vblex BARRIER Cnj_Rel) ;
% \end{verbatim}
% \end{itemize}

% Based on the comments in the file, the grammar writer wants to
% disambiguate the word form ``como'', which can be either a
% preposition, adverb or the first person singular of the verb
% ``comer''. 
% However, the rule here addresses the \emph{lemma} ``como'' instead of the
% word form, hence the SAT-solver cannot create a solution where
% \texttt{vblex} and ``como'' as a lemma are in the same
% reading. 
% A precise definition for this rule would target \texttt{vblex "<como>"} 
% or \texttt{vblex "comer"}, so that it becomes obvious that those two
% are supposed to be in the same reading. 

% Currently, as all the word forms are allowed to combine with anything,
% the ambiguity class constraint does not point this
% The automated 0-to-target conversion could come in handy here too.
% However, if the 

% TODO An example of the same but in condition. that the ambiguity class constraint, when
% implemented properly, should find out.



\begin{itemize}
\item[]\begin{verbatim}
REMOVE Sentar IF (0 Sentar) (..) ;

SELECT PP IF (0 "estado") (..) ;
\end{verbatim}
\end{itemize}

The comments make it clear that the first rule is meant to disambiguate between \emph{sentar} and \emph{sentir}, but the rule does not mention anything about \emph{sentir}.
Even with the ambiguity class constraints, the SAT-solver only created an ambiguity where \emph{sentar} in 1st person plural is ambiguous with an anonymous 1st person plural reading.
This does not reflect the reality, where the target is only ambiguous with certain verbs, and in certain conjugated forms.

The second case is potentially more dangerous. 
The word form $estado_{W}$ 
can be either a noun ($estado_{L}$, `state'), or the past participle of the verb $estar_{L}$. 
The condition, however, addresses the lemma of the noun, $estado_L$, whereas the lemma of the PP is $estar_{L}$.
This means that, in theory, there can be a case where the condition to select the PP is already removed. As for now, the lexicon does not contain other ambiguities with the word form $estado_{W}$, but we could conceive of a scenario where someone adds e.g. a proper noun $Estado_{L}$ to the lexicon. Then, if some rule removes the lemma $estado_{L}$, the rule to select PP will not be able to trigger.


Another question is whether this level of detail is necessary. 
After all, the grammar will be used to disambiguate real life texts, where \emph{sentar} will likely not have any other ambiguities, and aside from our contrived example, neither will \emph{estado}. 
In fact, we are planning to change how we handle the lexical forms; with those changes, it will become clear whether the imprecision will result in potential errors, given the current lexicon. 


% shows a shortcoming in our way of handling the lexical conditions. 
% If the lemmas and word forms were integrated to the analyses in a more
% sensible way, then the ambiguity class constraints would create a
% sequence where \emph{podar} was ambiguous with \emph{poder} only in the
% word form \emph{puedo}.

% instead of randomly chosen readings. So, even though this was not marked as a conflict, we can say that the weird output of the tool was unnecessary, and by better implementation, the tool would have created more realistic example sequence.


\begin{table*}[t!]
\centering
\begin{tabular}{|p{3.15cm}|p{2.55cm}|p{2.55cm}|p{2.7cm}|p{2.1cm}|}

\hline
              & 1 clitic + 
                lemmas from 
                 grammar & 2 clitics 
                           + lemmas from 
                              grammar & 1 clitic +
                                          all readings 
                                          from grammar    & all readings from grammar \\ \hline
\# readings   & 5851 (4263~+~1588)
                       & 9494
                       (7906~+~1588) & 6657 (4263~+~2394) & 2394  \\ \hline
\#~conflicts  & 214    & 214         & 22                 & 22 \\
(internal + 
 interaction) & (211~+~3) & (211~+~3) & (19~+~3)        &  (19~+~3)  \\ \hline

\clock{} all 
       rules & 2h 2min     & 4h 27min    & Xh Ymin &  Xh Ymin \\ \hline


\end{tabular}
\caption{Results for Finnish (1185 rules).}
\label{table:resFin}
\end{table*}

\subsection{Finnish} 

The results for the Finnish grammar are shown separately, in Table~\ref{table:resFin}. We encountered a number of difficulties and used a few shortcuts, which we did not need for the other grammars---most importantly, not using the ambiguity class constraints. Due to these complications, the results are not directly comparable, but we include Finnish in any case, 
to give an idea how our method scales up: both to more rules, and more complex rules.

\paragraph{Challenges with Finnish} The first challenge is the morphological complexity of Finnish.
There are more than 20,000 readings, when all possible clitic combinations are included.
After weeding out the most uncommon combinations, we ended up with sets of 4000--8000 readings.

The second challenge comes from the larger size of the grammar. Whereas the Spanish and Dutch had only tens of word forms or lemmas, the Finnish grammar specifies around 900 of them.
Due to both of these factors, the procedure described in Section~\ref{sec:realistic_readings} would have exploded the number of readings, so we simply took the lemmas and word forms, and added them as single readings. 
In cases where they were combined with another tag in the grammar, we took that reading directly: for instance, we included both \emph{aika} and ``\emph{aika} n'' from the rule \texttt{SELECT "aika" + N},
but nothing from the rule \texttt{SELECT Pron + Sg}. This method gave us 1588 additional readings.

Finally, we were not able to create ambiguity class constraints---expanding the Finnish morphological lexicon results in 100s of gigabytes of word forms, which is simply too big for our method to work. 
For future development, we will see if it is possible to manipulate the finite automata directly, instead of relying on the output in text.


\paragraph{Results}
The results are shown in Table~\ref{table:resFin}.
In the first column, we included only possessive suffixes. In the second column, we included question clitics as well.
Both of these readings include the 1588 lemmas and word forms from the grammar.
In the third column, we included all the tag combinations specified in the grammar, and in the fourth, we took only those, ignoring the morphological lexicon.

The first two variants contain a high number of internal conflicts. 
These are almost all due to nonexisting tags. The grammar was written in 1995, and updated by \newcite{pirinen2015}; such a high number of internal conflicts indicates that possibly something has gone wrong in the conversion, or in our expansion of the morphological lexicon.
As for accuracy, adding the question clitics did not change anything: they were already included in some of the 1588 sets with word forms or lemmas, and that was enough for the SAT-solver to find models with question clitics.
We left the result in the table just to demonstrate the change in speed.

%Example reading that adds question clitic to our arsenal: \texttt{SELECT "joka" + Pron + Q}

The second two variants are playing with the full set of readings from the grammar. For both of these, the number of reported conflicts was only 22.
Given the preliminary nature of the results, we did not do a full analysis of all the 214 reported conflicts.
Out of the 22, we found 17 of them as true conflicts,
 but 5 seemed to be caused by our handling of rules with \verb!*!: all of these 5 rules contain a LINK and multiple \verb!*!s. On a positive note, our naive handling of the \verb!*! seems to cover the simplest cases.





Some examples of true positives are shown in the following.

%SELECT ("oma" gen) IF (NOT 1 n|adj|vblex|vaux|pron|cs|cc|adp|post|pr|intj|num|abbr) (0 LINK *-1 "olla"|"voida"|"saattaa") (0 n|adj|vblex|vaux|pron|cs|cc|adp|post|pr|intj|num|abbr) (0C nom)

\begin{itemize}
\item[]\begin{verbatim}
"oma" SELECT Gen IF (..) (0C Nom) ;

SELECT Adv IF (NOT 0 PP) (..) ;
\end{verbatim}
\end{itemize}

Both of these are internal conflicts, which may not be trivial to see.
The first rule requires the target to be genitive and unambiguously nominative; however, these two tags cannot combine in the same reading.
As for the second rule, the definition of \texttt{PP} includes \texttt{adv} among others---with the sets expanded, this rule becomes \texttt{SELECT adv IF (NOT 0 pp|adv|adp|po|pr) (...)}.

The following two examples are interaction conflicts:

\begin{itemize}
\item[]\begin{verbatim}
REMOVE A (0 Der) ; 
REMOVE N (0 Der) ; 
REMOVE A/N (0 Der) ; 
\end{verbatim}
\end{itemize}

This is the same pattern we have already seen before, but with a set of rules as the reason for conflict.
The first two rules together remove the target of the third, leaving no way for there to be adjective or noun.

\begin{itemize}
\item[]\begin{verbatim}
SELECT .. IF (-1 Comma/N/Pron/Q) ;
SELECT .. IF (-2 ..) (-1 Comma) ;
\end{verbatim}
\end{itemize}

The rules above have been simplified to show only the relevant part.
The conflict lies in the fact that \texttt{Comma} is a subset of \texttt{Comma/N/Pron/Q}:
there is no way to trigger the second rule without placing a comma in position -1, and thereby triggering the first rule.




\subsection{Performance} 
The running time of the grammars ranges from seconds to hours. 
As can be seen from the times, increasing the number of readings
matters more than increasing the number of rules.
In addition, the ambiguity class constraints increase the time.


However, many of the use cases do not require running the whole
grammar. Testing the interaction between 5--10 rules takes just
seconds in all languages, if the ambiguity class constraints are not included. 
A downside in the ambiguity classes is that generating them takes a long time, 
and while the overhead may be acceptable when checking the full grammar,
it is hardly so when analysing just a handful of rules.


%Spanish lemmahack: time to create the formulas 5m32.214s

%Spanish no lemmahack: time to create the formulas  62m6.049s

%We believe we can still signifcantly improve the performance of the SAT-encoding. Currently it is just a naive proof-of-concept implementation, and we have not tried to optimise for speed at all.

% A Finnish symbolic word has around 5700 variables in the beginning of the analysis, each representing a possible tag combination.
% Around 3700 come from the Apertium morphological lexicon, excluding clitics, and around 2000 are defined in the grammar, as a target (\texttt{REMOVE ("bear" verb) IF ...}) or as a condition (\texttt{REMOVE ... IF (-1 ("bear" noun))}. The majority of them are lexical, whereas we don't include anything but morphological tags from the lexicon.
% For Spanish and Dutch, there are around 2000 and 300 tag combinations respectively.
% The Spanish lexicon includes up to two clitics (e.g. \emph{dámelo} `give.me.it') in the same word form, but Dutch has none in the lexicon.

% The SAT problem for checking one rule grows depending on how many rules come before it, how long the context is, and how many tag combinations are there. The running time of the last rule gives some indication ...


% Not all questions are so expensive.
% For instance, checking the tag sets in the Finnish grammar doesn't require the context of the other rules at all,
%  The majority of the correctly detected errors in the Finnish grammar were rule-internal---these errors don't need the context of the other rules at all.


\section{Conclusions and Future Work}
\label{sec:conclusion}

We set out to design and implement an automatic analysis of constraint grammars that can find problematic rules and rule combinations, without the need for a corpus.
Our evaluation indicates that the tool indeed finds non-trivial conflicts and dead rules
from actual grammars. 

We did not have a volunteer to test the tool in
the process of grammar writing, so we cannot conclude whether the
constructed examples are useful for getting new insights on the rules.
In any case, there are still a number of features to improve and add.

\paragraph{Combining morphological and lexical tags}

Our solution to hardcode the tag combinations in the readings is
feasible for simple morphology, but it can cause problems with more
complex morphology.
A more scalable solution would be to make each tag a variable, and ask the
question ``can this reading be a noun? how about singular? how about
conditional?'' separately for each tag. Then we could lift the
restriction of tag combinations into a SAT solver: make SAT clauses
that prohibit a comparative to go with a verb, or conditional with a noun.

We are currently working on adding the concepts of lemmas and word
forms directly to the representation of the possible readings.
Currently, if we want to add one more lemma for a verb, we need to
create as many new variables as there are distinct word forms---easily
hundreds for languages with rich morphology. 


\paragraph{Full expressivity of CG-3}
As for longer-term goals, we want to handle the full expressivity of CG-3,
with \textsc{map}, \textsc{add},  \textsc{addreading} and
\textsc{substitute} rules, as well as dependency structure. 
This also means finding different kinds of conflicts.
In order to implement rules that may add new readings, or new tags to
existing readings, we need to modify our approach in the SAT-encoding.
Even if the lexicon gives all readings that exist in the lexicon, the
user might give a nonexistent reading, or in the case of MAP, a
syntactic tag, which is (by definition) not in the lexicon. We may need to move
to a more scalable solution.


% \paragraph{Regular expressions}
% For rules with regular expressions, this would blow up even more: there are rules that only address whether the word form in a condition starts with a lowercase letter, or whether it ends in a certain suffix. A realistic, albeit unsatisfactory option is just to treat it as an underspecified reading, and offer that same regular expression to the user when generating a symbolic sentence. Another, less feasible, solution is to grep all the word forms or lemmas that match the regular expression in question---in the worst case (e.g. lower case), this will match all the entries in the expanded lexicon.

\paragraph{Support for grammar writers}
As mentioned earlier, we could support additional checks for common issues,
such as conditions that concern the target word.
Another possible feature is to suggest reformattings for a rule. Recall
Figure~\ref{fig:regroup} from the introduction; in that case, the
original rule was written by the original author, and another
grammarian thought that the latter form is nicer to read. Doing the
reverse operation could also be possible. If a rule with long
disjunctions conflicts, it may be useful to split it into smaller
conditions, and eliminate one at a time, in order to find the
reason(s) for the conflict.
Our next step is to evaluate our tools together with actual grammar writers,
in comparison with a corpus-based method or machine learning.

\paragraph{Other grammar formalisms}
Finally, we would like to investigate logic-based methods for analysing other grammar
formalisms, for example Grammatical Framework \cite{ranta2010gfbook}.

%\subsection{Conclusions}

%It is nice!

% * Preliminary results
%  - dutch & spanish
%  - mention scalability
%  - talk about size of SAT problem -- give number of SAT clauses for the last rule in the biggest grammar I have

% * Future work
%  - analysing different grammar formalisms
%  - asking different questions
%  - restrict yourself to readings that are actually words
