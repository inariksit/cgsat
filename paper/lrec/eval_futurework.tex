\section{Evaluation and Future Work}
\label{sec:eval}

We tested three grammars to find rules that cannot apply. The smallest grammar was Dutch\footnote{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-nld/apertium-nld.nld.rlx}}, with 59 rules; second was Spanish\footnote{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-spa/apertium-spa.spa.rlx}}with 279 rules, and the largest was Finnish\footnote{\url{https://github.com/flammie/omorfi/blob/master/src/vislcg3/omorfi.cg3}}, with 1185 rules. We left out rule types introduced in CG-3, and only tested the traditional \textsc{remove} and \textsc{select} rules.

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
                      & \textsc{nld}  & \textsc{spa}  & \textsc{fin}  \\ \hline
Rules in grammar      & 59              & 279               & 1185     \\ \hline
Conflicts found       & 1               & 1                 & ???    \\ \hline
Time: all rules       & 7s              & 3min 22s          & 3h 26min    \\ \hline
Time: last rule       & 4s              & 7s                & 3min 40s    \\ \hline
\# variables: last rule & 33915         & 247153            &    \\ \hline
%Time: longest rule    & ?s              & ?s                & ?min     \\ \hline
\end{tabular}
\caption{Results}
\label{table:res}
\end{table}

\paragraph{Accuracy} We tested all three grammars if they contain non-applicable rules:
for each rule, apply all the preceding rules to the symbolic sentence, and then ask if that last rule can fire.
This test revealed problems in all grammars, and for the smaller grammars, we were able to verify manually that the results are true.

The Dutch grammar had the following rules in the given order, making the latter impossible to apply:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE Adv IF (1 Noun) ;
REMOVE Adv IF (-1 Det) (0 Adj) (1 Noun) ;
\end{verbatim}
\end{itemize}

The Spanish grammar had an erroneous set definition, where a word was required to have almost all POS tags at the same time. After fixing the set definition, our test didn't reveal other problems.
The program marked \todo{n} problem rules for the Finnish grammar, but we didn't have time to verify if all of them were genuinely problematic or if our program is at fault.
Correct errors include \todo{find a real problem and describe shortly}.
Our program detected tagset errors also in the Finnish grammar.
The grammar was originally written in 1995, and converted to the Omorfi tagset by \cite{pirinen2015}. 
For example, the Omorfi tagset marks a possessive suffix with \texttt{Px}, e.g. \texttt{PxPl2} for second person plural, but the grammar used the old convention \texttt{PL2} in the rules.
As a result, all rules with possessive suffixes were detected as impossible.
In addition, we suspect that some of the problems come just from our naive implementation.
For a condition that states $n$ or more words to the left or right, we only create a word in the sentence exactly $n$ words away. This is something to be fixed in the future.

 at least part of the result is related to the length of the sentences---for a condition that states $n$ or more words to the left or right, we only create a word exactly $n$ words away. This is something to be fixed in the future.

\paragraph{Running time} The speed of the check is an obvious issue currently.
The Finnish symbolic word has around 5700 variables in the beginning of the analysis, each representing a possible tag combination.
Around 3700 come from the Apertium morphological lexicon, excluding clitics, and around 2000 are present in the grammar---many of them include an actual word, both in target (\texttt{REMOVE (``bear'' verb) IF ...}) and in conditions (\texttt{REMOVE ... IF (-1 (``bear'' noun))}.

For Spanish and Dutch, the numbers are around 2000 and 300 respectively.
The Spanish lexicon includes up to two clitics (e.g. \emph{d√°melo} `give.me.it') in the same word form, but Dutch has none in the lexicon.

%All rules is the total time to check every rule against the previous rules, and last rule shows how long it took to check the last rule.
As indicated by the total time and the time to check the last rule, the size of the SAT problem grows according to how far in the sequence the rule is.

As seen in table~\ref{table:res}, checking every rule in the Finnish grammar takes currently 3.5 hours.
A rule takes longer to run if it appears later in the rule sequence: 1000 previous rules is a larger SAT problem than 10 previous rules.
This can be seen from the time it took to compute the result for the last rule. For Finnish, the last rule took almost 4 minutes, but if we divide the time with the number of rules, the average time for one rule is just 10 seconds.
We also measured the running times for the longest rules in the grammar.
\todo{find longest rule, change place of that rule, see what matters more. Promise: we can be smarter about SAT things.}


\paragraph{Future work} We will continue improving the performance, and make sure that the implementation for the basic rules is correct; for instance, fixing the earlier mentioned issue with a non-exact position.
After this, we want to handle the full expressivity of CG-3, with \textsc{map}, \textsc{add} and \textsc{substitute} rules.
At that stage, it would be interesting to test our program with actual grammar writers,
in comparison with a corpus-based method or machine learning.

As another line of investigation, we want to detect more kinds of conflicts.
For instance, say we have the following rules:

\begin{itemize}
\item[] \begin{verbatim}
REMOVE verb IF (-1C det) ;
REMOVE noun IF (-1C det) ;
\end{verbatim}
\end{itemize}

With our symbolic sentence, these rules will be no problem; to apply the latter, we only need to construct a target that has a realistic ambiguity with a noun; the verb is gone already.
However, in the context of real life texts, the grammar writer might actually mean the following:

\begin{itemize}
\item[] \begin{verbatim}
REMOVE verb IF (-1C det) (0 noun) ;
REMOVE noun IF (-1C det) (0 verb) ;
\end{verbatim}
\end{itemize}

In that case, those rules are contradictory, and we would like to find it out.

Finally, we want to test the approach to other grammar formalisms.



% * Preliminary results
%  - dutch & spanish
%  - mention scalability
%  - talk about size of SAT problem -- give number of SAT clauses for the last rule in the biggest grammar I have

% * Future work
%  - analysing different grammar formalisms
%  - asking different questions
%  - restrict yourself to readings that are actually words