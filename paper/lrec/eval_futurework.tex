\section{Evaluation}
\label{sec:eval}

We tested three grammars to find conflicting rules: 
Dutch\footnote{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-nld/apertium-nld.nld.rlx}},
with 59 rules; 
Spanish\footnote{\url{https://svn.code.sf.net/p/apertium/svn/languages/apertium-spa/apertium-spa.spa.rlx}},
with 279 rules; and 
Finnish\footnote{\url{https://github.com/flammie/omorfi/blob/master/src/vislcg3/omorfi.cg3}},
with 1185 rules. We left out \textsc{add}, \textsc{map} and other rule
types introduced in CG-3, and only tested \textsc{remove} and \textsc{select} rules.
The results are shown in table~\ref{table:res}. 


\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
              & \textsc{nld}  & \textsc{spa}  & \textsc{fin}  \\ \hline
\# rules      & 59              & 279      & 1185     \\ \hline
\# readings   & 336             & 2191     & 2012    \\ \hline
\# conflicts  & 8 (1 + 5 + 2)   & 1        & 51    \\ \hline
\clock{} all rules       & 7s              & 3min 22s    & 3h 26min    \\ \hline
\clock{} last rule       & 4s              & 7s          & 3min 40s    \\ \hline
% This is with just 2000ish variables for Finnish, below is with 5700

% \clock{} last rule       & 4s              & 7s          & 30min 25s    \\ \hline
% \clock{} longest rule    & ?s              & ?s                & ?min     \\ \hline
%\# variables: last rule & 33915         & 247153            &    \\ \hline

\end{tabular}
\caption{}
\label{table:res}
\end{table}

\paragraph{Accuracy} 
The experiments revealed problems in all grammars. For the smaller
grammars, we were able to verify manually that the detected rules were
true positives.
%The results for the Finnish grammar are inconclusive, 
% For the Finnish grammar, we did not have time to
% investigate all of them, but among those we could, we found both true and false negatives.
We did not check for false negatives in any of the grammars. However, the test reported a high number of errors even in the smallest grammars, and some of them are intuitively not that bad; we 


\paragraph{Dutch} The Dutch grammar had two kinds of errors: rule-internal due to nonexistent tags, and rule interaction. As for rule-internal conflicts, one was due to a misspelling in the list definition for personal pronouns, which rendered 5 rules ineffective. The other was about subreadings: the genitive \emph{s} is analysed as a subreading in the Apertium morphological analyser, but it appeared in 2 rules as the main reading. 

There were two genuine conflicts with rule interaction. The first one is shown below:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE Adv IF (1 Noun) ;
REMOVE Adv IF (-1 Det) (0 Adj) (1 Noun) ;
\end{verbatim}
\end{itemize}

The first one is more straight-forward: two rules, which both remove an adverb, were in an order where the first has more broad condition: remove adverb if followed by a noun. In contrast, the second rule has a stricter requirement: only remove adverb if it is preceded by a determiner; the adverb itself is ambiguous with an adjective, and followed by a noun. The proble is that the first rule removes the adverb in all possible cases, and the second will not have any chance to act. If the rules were in the opposite order, then there would be no problem.


The second conflict, shown below, is more subtle. 

\begin{itemize}
\item[] 
\begin{verbatim}
SELECT DetPosNotZijn IF (1 Noun) ;
SELECT DetPos IF (-1 (vbser pres p3 sg)) 
                 (0 "zijn") (1 Noun);
\end{verbatim}
\end{itemize}

The condition of the second rule cannot be true: there can never be a ``zijn'' in the target, because the previous rule has selected everything that is a possessive determiner and not ``zijn''. 
%Not firing is not an option: if conditions of the first rule do not hold, then they will not hold for the second either. If the target is the only left, then 
% "target" in case of SELECT means complement of the mentioned target. so 
%    some of [adj, det pos zijn, verb zijn, punct, ...]  is true
%    all of  [det pos f sg, det pos m pl, det pos foo bar] are false.
% ie. there has been a rule that has removed detPosNotZijn, or they haven't been there in the first place.
% 

Another remark on this formulation of a rule. 
As per VISL CG-3, the condition \texttt{(0 "zijn")} does not require that the input would have determiner and ``zijn'' in the same reading: just that there is any determiner and any ``zijn'' in the cohort. 
The first example of a rule conflict had a similar requirement, but used correctly: the intended semantics was indeed that the adjective and the adverb are in different readings. However, in this case, the correct way to express the target is \texttt{SELECT DetPos + "zijn"}.

Can we catch this additional problem with our tool? The example word constructed by the SAT-solver may as likely be one of the following:

\begin{itemize}
\item[a.] \begin{verbatim}
"w2"
    w2<det><pos><f><sg>
    w2<vbser><inf><"zijn">
\end{verbatim}

\item[b.] \begin{verbatim}
"w2"
    w2<det><pos><mfn><pl><"zijn">
\end{verbatim}
\end{itemize}

In the latter case, the grammar writer would hopefully notice the imprecise definition and change their definition. However, this is more of a happy side effect than intended feature---the grammar writer cannot count on our tool to notice all similar cases.
We could add a feature that alerts in all cases where a condition with 0 is used, or even automatically construct a version of the rule that moves whatever tags in the 0-condition into the target.

\paragraph{Spanish} The Spanish grammar had proportionately the highest number of errors.\footnote{TODO://link/to/error.log} \todo{confirm; I assume this is gonna be so, but let's see Fin and Por}.  The grammar we ran is like the one found in the Apertium repository, apart from 2 changes: we fixed some typos (capital O for 0) in order to make it compile, and commented out two rules that used regular expressions, because our solution does not handle them properly (see more on Future work/Implementation).

Again, we can classify the conflicts into internal and interaction. As an example of internal conflict, there are \todo{n} rules that use \texttt{SET Cog = (np cog)}---the problem is that the tag \texttt{cog} does not exist in the Apertium dictionary for Spanish. It is likely that this grammar has been written for an earlier version, where such tag has been in place.

The Spanish grammar has a high number of interaction conflicts, all with the same pattern:

\begin{itemize}
\item[] 
\begin{verbatim}REMOVE (a) IF (-1 x) (1 y) ;
REMOVE (a) IF (-1 x) (1 y) (NOT -1 z);
\end{verbatim}
\end{itemize}

This is another, perhaps counter-intuitive instance of more general rule applying before more specific. Negated context works as well to make a rule stricter. The second rule will never apply, because the first will catch all cases.

In addition, there was a number of definitions that were never used. The program doesn't currently point them out to the user, but that is a trivial addition; just like rule-internal conflicts, it doesn't require any semantic analysis. 
Another error, which was not pointed by the program, but discovered 


\paragraph{Finnish} \todo{Rerun the tests and rewrite this whole section!} For the Finnish grammar, we did not have time to investigate all the
results. We could verify approximately \nicefrac{1}{3} as true positives and
\nicefrac{1}{3} as false positives.
The verified true positives were all rule-internal conflicts. 
For example, the following rule requires the target to be genitive and unambiguously nominative: 
\begin{itemize}
\item[]\texttt{"oma" SELECT GEN IF [...] (0C NOM) ;}
\end{itemize}
We also detected a number of tagset errors;
the grammar was originally written in 1995, and converted to the
Omorfi tagset by \cite{pirinen2015}, but there were 15 rules that used the old convention for possessive suffixes.

%  For example, the Omorfi tagset
% marks a possessive suffix with \texttt{Px}, e.g. \texttt{PxPl2} for second person plural, but the
% grammar uses the old convention \texttt{PL2} in the rules.
% As a result, 15 rules with possessive suffixes were detected as
% impossible.
To improve performance, we ignored the fully specified readings from the
lexicon and used only tag combinations from the grammar:
for example, the rule \texttt{REMOVE (verb sg) IF (-1 det)}
gives us ``verb sg'' and ``det''.
The shortcut works most of the time, but it is possible to
miss some cases: e.g. \texttt{SELECT PRON + REL IF (0 NOM)} 
assumes ``pron rel nom'', but our method only gives
``pron rel'' and ``nom'' separately. 
17 false positives were caused by this shortcut. 
In addition, 2 false positives were caused by the naive implementation of
rules with \verb!*! in their context.

Due to these reasons, the results about the Finnish grammar are
inconclusive. We only include them to show some preliminary data on larger grammars.
%For a condition situated $n$ or more words to the left or right, we only create a word exactly $n$ words away. This is something to be fixed in the future.

\paragraph{Performance} The speed of the check is an obvious issue,
especially for the Finnish grammar. 
As mentioned before, we used 2000 readings extracted from the grammar.
With the full set of 6000 readings from the lexicon, the speed
was even worse: checking the last rule took 30 minutes, as opposed to
3 minutes with 2000 readings. 
%We never finished running the whole rule set with 6000 readings, but
%it would have taken potentially days.
We believe we can still signifcantly improve the performance of the SAT-encoding. Currently it is just a naive proof-of-concept implementation, and we have not tried to optimise for speed at all.

% A Finnish symbolic word has around 5700 variables in the beginning of the analysis, each representing a possible tag combination.
% Around 3700 come from the Apertium morphological lexicon, excluding clitics, and around 2000 are defined in the grammar, as a target (\texttt{REMOVE ("bear" verb) IF ...}) or as a condition (\texttt{REMOVE ... IF (-1 ("bear" noun))}. The majority of them are lexical, whereas we don't include anything but morphological tags from the lexicon.
% For Spanish and Dutch, there are around 2000 and 300 tag combinations respectively.
% The Spanish lexicon includes up to two clitics (e.g. \emph{dámelo} `give.me.it') in the same word form, but Dutch has none in the lexicon.

% The SAT problem for checking one rule grows depending on how many rules come before it, how long the context is, and how many tag combinations are there. The running time of the last rule gives some indication ...


% Not all questions are so expensive.
% For instance, checking the tag sets in the Finnish grammar doesn't require the context of the other rules at all,
%  The majority of the correctly detected errors in the Finnish grammar were rule-internal---these errors don't need the context of the other rules at all.


\section{Future work}


Our solution to hardcode the tag combinations in the readings is feasible for simple morphology, but it can cause problems with more complex morphology. One big downside is that in order to implement \textsc{ADD}, \textsc{ADDREADING} and \textsc{MAP}, we need to be prepared for new readings--even if the lexicon gives all readings that exist in the lexicon, the user might give a nonexistent reading, or in the case of MAP, a syntactic tag, which is (by definition) not in the lexicon. A more scalable solution would be to make each tag a variable, and ask the question ``can this reading be a noun? how about singular? how about conditional?'' separately for each tag. Then we could lift the restriction of tag combinations into a SAT solver: make SAT clauses that prohibit a comparative to go with a verb, or conditional with a noun.
Another benefit of this solution is the addition of lemmas and word forms, as well as regular expressions: currently, if we want to add one more lemma for a verb, we need to create as many new variables as there are distinct verb forms---easily hundreds for languages with rich morphology.
\todo{less rambling: For rules with regular expressions, this would blow up even more: there are rules that only address whether the word form in a condition starts with a lowercase letter, or whether it ends in a certain suffix. A realistic, albeit unsatisfactory option is just to treat it as an underspecified reading, and offer that same regular expression to the user when generating a symbolic sentence. Another, less feasible, solution is to grep all the word forms or lemmas that match the regular expression in question---in the worst case (e.g. lower case), this will match all the entries in the expanded lexicon.}


As for longer-term goals, we want to handle the full expressivity of CG-3,
with \textsc{map}, \textsc{add} and \textsc{substitute} rules, and
dependency structure. This also means finding different kinds of conflicts.
As soon as the tools are mature enough, we want to
evaluate them with actual grammar writers,
in comparison with a corpus-based method or machine learning.
Finally, if the method proves feasible for CG, we want
to try applying it to other grammar formalisms.


% * Preliminary results
%  - dutch & spanish
%  - mention scalability
%  - talk about size of SAT problem -- give number of SAT clauses for the last rule in the biggest grammar I have

% * Future work
%  - analysing different grammar formalisms
%  - asking different questions
%  - restrict yourself to readings that are actually words
