\section{Implementation}
\label{sec:implementation}

In this section, we describe the implementation of the tool.
The SAT-encoding we use is similar to the one introduced in \newcite{listenmaa_claessen2015}, with one key difference: in this paper, we operate on {\em symbolic sentences} instead of concrete sentences from a corpus. The idea is that the SAT-solver is going to find the concrete sentence for us.

%\paragraph
\subsection{Preliminaries}

Our analysis operates on a rule $r$, which is preceded by a list of rules $R$, and is concerned with answering the following question: ``Does there exist an input sentence $S$ that can trigger rule $r$, even after passing all rules $R$ that came before $r$?''

Before we can do any analysis any of the rules, we need to find out what the set of all possible readings of a word is. We can do this by extracting this information from a lexicon, but there are other ways too. In our experiments, the number of readings has ranged from about 300 to about 6000. 

Furthermore, when we analyse a rule $r$, we need to decide the {\em width} $w(r)$ of the rule $r$: How many different words should there be in a sentence that can trigger $r$? Most often, $w(r)$ can be easily determined by looking at how far away the rule context indexes in the sentence relative to the target. For example, in the rule mentioned in the introduction, the width is 2.

If the context contains a \verb!*! (context word can be anywhere),
we may need to make an approximation of $w(r)$ which may result in false negatives later on in the analysis.
%we create sentences that are up to 3 words wider than the context without \verb!*!, and try them all.

%\paragraph
\subsection{Symbolic sentences}

We start each analysis by creating a so-called {\em symbolic sentence}, which is our representation of the sentence $S$ we are looking for. A symbolic sentence is a sequence of {\em symbolic words}; a symbolic word is a table of all possible readings that a word can have, where each reading is paired up with a SAT-variable.

The number of words in the symbolic sentence we create when we analyse a rule $r$ is $w(r)$. For the rule in the introduction, we have $w(r)=2$ and a symbolic sentence may look as follows:
\begin{center}
\begin{tabular}{c|c|c}
word1 & word2 & reading \\
\hline
$v_1$ & $w_1$ & det def \\
$v_2$ & $w_2$ & noun sg \\
$v_3$ & $w_3$ & noun pl \\
$v_4$ & $w_4$ & verb sg \\
$v_5$ & $w_5$ & verb pl \\
\end{tabular}
\end{center}
Here, $v_i$ and $w_j$ are SAT-variables belonging to word1 and word2, respectively. We can also see that the possible number of readings here was 5.

The SAT-solver contains extra constraints about the variables. Input sentences should have at least one reading per word, so we add the following two constraints:
\begin{center}
\begin{tabular}{c}
$v_1 \vee v_2 \vee v_3 \vee v_4 \vee v_5$, \\
$w_1 \vee w_2 \vee w_3 \vee w_4 \vee w_5$ \\
\end{tabular}
\end{center}
Any solution to the constraints found by the SAT-solver can be interpreted as a concrete sentence with $w(r)$ words that each have a set of readings.

%\paragraph
\subsection{Applying a rule}

Next, we need to be able to apply any given rule $r'$ to a symbolic sentence, resulting in a new symbolic sentence.

For example, if we apply the rule from the introduction to the symbolic sentence above, the result is the following symbolic sentence:
\begin{center}
\begin{tabular}{c|c|c}
word1 & word2 & reading \\
\hline
$v_1$ & $w_1$ & det def \\
$v_2$ & $w_2$ & noun sg \\
$v_3$ & $w_3$ & noun pl \\
$v_4$ & $w_4'$ & verb sg \\
$v_5$ & $w_5'$ & verb pl \\
\end{tabular}
\end{center}
The example rule can only affect readings of word2 that have a ``verb'' tag, so we create only two new variables $w_4'$ and $w_5'$ for the result, and reuse the other variables. We must also add the following constraint for $w_4'$:
\begin{center}
\begin{tabular}{c}
$w_4' \Leftrightarrow [ w_4 \wedge \neg{}(v_1 \wedge (w_1 \vee w_2 \vee w_3)) ]$ \\
\end{tabular}
\end{center}
In other words, after applying the rule, the reading ``verb sg'' (represented by the variable $w_4'$) can only be in the resulting sentence exactly when (1) ``verb sg'' was a reading of the input sentence (so $w_4$ is true) and (2) the rule has not been triggered (the rule triggers when $v_1$ is true and at least one of the non-verb readings $w_1 \dots w_3$ is true). We add a similar constraint for the new variable $w_5'$:
\begin{center}
\begin{tabular}{c}
$w_5' \Leftrightarrow [ w_5 \wedge \neg{}(v_1 \wedge (w_1 \vee w_2 \vee w_3)) ]$ \\
\end{tabular}
\end{center}

%\paragraph
\subsection{Putting it all together}

Once we know how to apply any rule $r'$ to a symbolic sentence, resulting in a new symbolic sentence, we can apply all rules preceding the rule $r$ that is under analysis. We simply apply each rule to the result of applying the previous rule. In this way, we end up with a symbolic sentence that represents all sentences that could be the result of applying all those rules.

Finally, we can take a look at the rule $r$ we want to analyse. Here is an example:
\begin{itemize}
\item[] \texttt{REMOVE det IF (1 verb) ;}
\end{itemize}
If we take the symbolic sentence above as input, we want to ask whether or not it can trigger the rule $r$. We do this by adding some more constraints to the SAT-solver.

First, the context of the rule should be applicable, meaning that the second word should have a reading with a ``verb'' tag:
\begin{center}
\begin{tabular}{c}
$w_4' \vee w_5'$
\end{tabular}
\end{center}
Second, the rule should be able to remove the ``det'' tag, meaning that the first word should have a reading with a ``det'' tag, and there should be at least one other reading:
\begin{center}
\begin{tabular}{c}
$v_1 \wedge (v_2 \vee v_3 \vee v_4 \vee v_5)$
\end{tabular}
\end{center}
If the SAT-solver can find a solution to all constraints generated so far, we have found a concrete sentence that satisfies our goal. If the SAT-solver cannot find a solution, it means that there are no sentences that can ever trigger rule $r$. (This means that there is something wrong with the grammar.)

%\paragraph{Creating realistic readings}
\subsection{Creating realistic readings}
\label{sec:realistic_readings}

Earlier we have shown an example with 5 readings (``det def'', ``noun sg'', ...). In a realistic case, we operate between hundreds and thousands of possible readings. 
%This is very much dependent on language: the simplest language we tested was Dutch, with 336 readings. The most complex was 
In order to find the set of readings, we expand a morphological lexicon\footnote{We used the lexica from Apertium, found in \url{https://svn.code.sf.net/p/apertium/svn/languages/}.}, ignore the word forms and lemmas, and take all distinct analyses. 
However, many grammar rules target a specific lemma or word form.
A simple solution is to retain the lemmas and word forms only for those entries where it is specified in the grammar, and otherwise leave them out. For example, the Dutch grammar contains the following rule:

\begin{itemize}
 \item[] \texttt{REMOVE ("zijn" vbser) IF (-1 Prep) (1 Noun) ;}
\end{itemize}

This hints that there is something special about the verb \emph{zijn}, compared to the other verbs. Looking at the lexicon, we find \emph{zijn} in the following entries:

\begin{itemize}
 \item[] 
\begin{verbatim}zijn:zijn<det><pos><mfn><pl>
zijn:zijn<det><pos><mfn><sg>
zijn:zijn<vbser><inf>
zijn:zijn<vbser><pres><pl>
\end{verbatim}
\end{itemize}

Thus we add special entries for these: in addition to the anonymous
``det pos mfn pl'' reading, we add ``\emph{zijn} det pos mfn pl''. 
The lemma is treated as just another tag.

 However, for languages with more readings, this may not be feasible. For instance, Spanish has a high number of readings, not only because of many inflectional forms, but because it is possible to add 1--2 clitics to the verb forms.
The number of verb readings without clitics is 213, and with clitics 1572.
With the previously mentioned approach, we would have to double 1572 entries for each verb lemma. Even ignoring the clitics, each verb lemma still adds 213 new readings.

The readings in a grammar can be underspecified: for example, the rule
\texttt{REMOVE (verb sg) IF (-1 det)} gives us ``verb sg'' and ``det''.
In contrast, the lexicon only gives us fully specified readings, such
as ``verb pres p2 sg''. We implemented a version where we took
the tag combinations specified in the grammar directly as our
readings, and we could insert them into the symbolic sentences as well.
The shortcut works most of the time, but if we only take the readings
from the grammar and ignore the lexicon, it is possible to
miss some cases: e.g. the rule \texttt{SELECT Pron + Rel IF (0 Nom)} 
may require ``pron rel nom'' in one reading, but this method only gives
``pron rel'' and ``nom'' separately. 
In addition, we found that the tag lists in the grammars sometimes
contain errors, such as using a nonexistent tag or using a wrong level
in a subreading. If we accept those lists as readings, we will
generate symbolic sentences that are impossible, and won't discover
the bug in the grammar.

However, if we only want to find rule interaction effects, then using
the underspecified readings from the grammar makes the task faster,
and it will still catch potential interaction errors.


\subsection{Creating realistic ambiguities}




In the previous section, we have created realistic \emph{readings}, by simply hardcoding legal tag combinations into variables. The next step in creating realistic \emph{ambiguities} is to constrain which readings can go together. For instance, the case of \emph{zijn} shows us that ``determiner or verb'', is a possible ambiguity. In contrast, there is no word form in the lexicon that would be ambiguous between an adjective and a comma, hence we do not want to generate such ambiguity in our symbolic sentences.

\begin{center}
\begin{tabular}{c|c|c|c|c}


            & n nt sg  & n f pl  & vblex sep inf & det pos mfn  \\ \hline
uitgaven    & 0        & 1       & 1             & 0    \\ 
toespraken  & 0        & 1       & 1             & 0    \\ 
haar        & 1        & 0       & 0             & 1    \\ 


\end{tabular}
\end{center}

We solve the problem by creating \emph{ambiguity classes}: groups of readings that can be ambiguous with each other. 
We represent the expanded morphological lexicon as a matrix, as seen
above: word forms on the rows and analyses on the columns. Each
distinct row forms an ambiguity class. For example, one class may
contain words that are ambiguous between plural feminine nouns and
separable verb infinitives; another contains masculine plural adjectives 
and masculine plural past participles.
Then we form SAT clauses that allow or prohibit certain combinations. These clauses will interact with the constraints created from the rules, and the end result will be closer to real-life sentences.

Our approach is similar to \newcite{cutting_etal92}, who use ambiguity classes instead of distinct word forms, in order to reduce the number of parameters in a Hidden Markov Model. They take advantage of the fact that they don't have to model ``bear'' and ``wish'' as separate entries, but they can just reduce it to ``word that can be ambiguous between noun and verb'', and use that as a parameter in their HMM. 
%We can do a similar thing by saving a list of words with each ambiguity class. For example, we map the ambiguity class ``feminine plural noun or a separable verb infinitive'' to the list of word forms \{``uitgaven'', ``toespraken''\}, and then, if we generate such reading for our symbolic word, we can give one of these words as an example word.

There are two advantages of restricting the ambiguity within words.
Firstly, we can create more realistic example sentences, which should help the grammar writer.
Secondly, we can possibly detect some more conflicts. Assume that the grammar contains the following rules:

 \begin{itemize}
 \item[] 
\begin{verbatim}
 REMOVE adj IF (-1 aux) ;
 REMOVE pp  IF (-1 aux) ;
 \end{verbatim}
 \end{itemize}

 With our symbolic sentence, these rules will be no problem; to apply the latter, we only need to construct a target that has a realistic ambiguity with a past participle; the adjective will be gone already.
However, it could be that past participles (pp) only ever get confused with adjectives---in that case, the above rules would contradict each other.
 By removing the adjective reading, the first rule selects the past participle reading, making it an instance of ``$r$ selects something in a context, $r'$ removes~it''. 
The additional constraints will prevent the SAT-solver from creating an ambiguity outside the allowed classes, and such a case would be caught as a conflict.


% \paragraph{Other questions we can ask}

% Apart from finding out what rules prevent others from applying, we can also find out if there is a conflict that is rule-internal, such as nonexisting tag set or contradicting requirements of a context word (e.g. a word must be unambiguously two different POS).

% In addition to analysing a whole grammar, 
% we can construct all kinds of symbolic sentences to test out individual rules. 
% We can set the length, restrict individual words (e.g. ``3rd word must be a noun''), require that it triggers some rule but not other. 
% This functionality can aid the grammar writer in the process, to see if they have missed a case or defined the tagset correctly.

