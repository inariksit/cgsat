\section{Implementation}
\label{sec:implementation}

In this section, we describe the implementation of the tool.
The SAT-encoding we use is similar to the one introduced in \cite{listenmaa_claessen2015}, with one key difference: in this paper, we operate on {\em symbolic sentences} instead of concrete sentences from a corpus. The idea is that the SAT-solver is going to find the concrete sentence for us.

\paragraph{Preliminaries}

Our analysis operates on one rule $R$, and is concerned with answering the following question: ``Does there exist an input sentence $S$ that can trigger rule $R$, even after passing all rules $R'$ that came before $R$?''

Before we can do any analysis any of the rules, we need to find out what the set of all possible readings of a word is. We can do this by extracting this information from a lexicon, but there are other ways too. In our experiments, the number of readings has ranged from about 300 to about 6000. 

Furthermore, when we analyse a rule $R$, we need to decide the {\em width} $w(R)$ of the rule $R$: How many different words should there be in a sentence that can trigger $R$? Most often, $w(R)$ can be easily determined by looking at how far away the rule context indexes in the sentence relative to the target. For example, in the rule mentioned in the introduction, the width is 2.

If the context contains a \verb!*!, we may need to make an approximation of $w(R)$ which may result in false negatives later on in the analysis.

\paragraph{Symbolic sentences}

We start each analysis by creating a so-called {\em symbolic sentence}, which is our representation of the sentence $S$ we are looking for. A symbolic sentence is a sequence of {\em symbolic words}; a symbolic word is a table of all possible readings that a word can have, where each reading is paired up with a SAT-variable.

The number of words in the symbolic sentence we create when we analyse a rule $R$ is $w(R)$. For the rule in the introduction, we have $w(R)=2$ and a symbolic sentence may look as follows:
\begin{center}
\begin{tabular}{c|c|c}
word1 & word2 & reading \\
\hline
$v_1$ & $w_1$ & det def \\
$v_2$ & $w_2$ & noun sg \\
$v_3$ & $w_3$ & noun pl \\
$v_4$ & $w_4$ & verb sg \\
$v_5$ & $w_5$ & verb pl \\
\end{tabular}
\end{center}
Here, $v_i$ and $w_j$ are SAT-variables belonging to word1 and word2, respectively. We can also see that the possible number of readings here was 5.

The SAT-solver contains extra constraints about the variables. Input sentences should have at least one reading per word, so we add the following two constraints:
\begin{center}
\begin{tabular}{c}
$v_1 \vee v_2 \vee v_3 \vee v_4 \vee v_5$, \\
$w_1 \vee w_2 \vee w_3 \vee w_4 \vee w_5$ \\
\end{tabular}
\end{center}
Any solution to the constraints found by the SAT-solver can be interpreted as a concrete sentence with $w(R)$ words that each have a set of readings.

\paragraph{Applying a rule}

Next, we need to be able to apply a given rule $R'$ to a symbolic sentence, resulting in a new symbolic sentence.

For example, if we apply the rule from the introduction to the symbolic sentence above, the result is the following symbolic sentence:
\begin{center}
\begin{tabular}{c|c|c}
word1 & word2 & reading \\
\hline
$v_1$ & $w_1$ & det def \\
$v_2$ & $w_2$ & noun sg \\
$v_3$ & $w_3$ & noun pl \\
$v_4$ & $w_4'$ & verb sg \\
$v_5$ & $w_5'$ & verb pl \\
\end{tabular}
\end{center}
The example rule can only affect readings of word2 that have a ``verb'' tag, so we create only two new variables $w_4'$ and $w_5'$ for the result, and reuse the other variables. We add the following constraint for $w_4'$:
\begin{center}
\begin{tabular}{c}
$w_4' \Leftrightarrow [ w_4 \wedge \neg{}(v_1 \wedge (w_1 \vee w_2 \vee w_3)) ]$ \\
\end{tabular}
\end{center}
In other words, after applying the rule, the reading ``verb sg'' (represented by the variable $w_4'$) can only be in the resulting sentence exactly when (1) ``verb sg'' was a reading of the input sentence (so $w_4$ is true) and (2) the rule has not been triggered (the rule triggers when $v_1$ is true and at least one of the non-verb readings $w_1 \dots w_3$ is true). We add a similar constraint for the new variable $w_5'$:
\begin{center}
\begin{tabular}{c}
$w_5' \Leftrightarrow [ w_5 \wedge \neg{}(v_1 \wedge (w_1 \vee w_2 \vee w_3)) ]$ \\
\end{tabular}
\end{center}

\paragraph{Choosing the readings}

Earlier we have shown an example with 5 readings (\texttt{det def}, \texttt{noun sg}, ...). In the real implementation, we operate between hundreds and thousands of readings. This is very much dependent on language: Dutch, the simplest language, has 336 readings.

We expand an Apertium morphological lexicon, throw away the word forms and take all the analyses. An exception is when the grammar contains rules that target a specific lemma or word form.
A simple solution is to retain the lemmas and word forms only for those entries where it is specified in the grammar, and otherwise leave them out. For example, the grammar may contain the following list:

\begin{itemize}
 \item[] \texttt{LIST ZijnVerb = ("zijn" vbser pres pl) ;}
\end{itemize}

Clearly there is something special about the verb \emph{zijn}, not just any verb. Looking at the lexicon, we find \emph{zijn} in the following entries:

\begin{itemize}
 \item[] \begin{verbatim}zijn:zijn<det><pos><mfn><pl>
zijn:zijn<det><pos><mfn><sg>
zijn:zijn<vbser><inf>
zijn:zijn<vbser><pres><pl>
\end{verbatim}
\end{itemize}

Thus we add special entries for these: in addition to the anonymous
 \texttt{<det><pos><mfn><pl>} reading, we add \texttt{<"zijn"><det><pos><mfn><pl>}.

 However, for languages with more readings, this may not be feasible. For instance, the Spanish lexicon has a high number of readings mostly because of the verbs: in addition to the verb forms, there are clitics attached to them. The number of verb readings without clitics is 213, and with clitics 1572.

 In our experience,  even ignoring the clitics doubles the amount of readings.

Another note: the readings in grammar can be underspecified (e.g. ``verb sg''), whereas the lexicon only gives us fully specified (``verb pres p2 sg'') readings. \todo{Is this suitable for implementation, or sh0ould it be in eval/discussion/something else?} We tried a version where we took the tag combinations specified in the grammar as readings, and we could insert them into the symbolic sentences as well, but this was not an ideal solution: turns out that the tag lists in the grammars contain often errors (e.g. replacing OR with an AND; using a nonexistent tag; using a wrong subreading number), and if we accept those lists as readings, we will generate symbolic sentences that are impossible, and won't discover the bug in the grammar.

However, if we only want to find rule interaction effects, then using the underspecified readings from the grammar makes the task faster, and it will still catch potential interaction errors.

All in all, our solution to hardcode the tag combinations in the readings is feasible for simple morphology, but it can cause problems with more complex morphology. One big downside is that in order to implement \textsc{ADD}, \textsc{ADDREADING} and \textsc{MAP}, we need to be prepared for new readings--even if the lexicon gives all readings that exist in the lexicon, the user might give a nonexistent reading, or in the case of MAP, a syntactic tag, which is (by definition) not in the lexicon.


\paragraph{Creating realistic ambiguities}

In the previous paragraph, we have created realistic \emph{readings}:  ``conditional'' can only go in verb readings, or ``comparative'' with adjective readings. This is done just by hardcoding those combinations into variables. The next step in creating realistic ambiguities is to constrain what readings can go together. For instance, the case of \emph{zijn} shows us that a determiner and a verb, in present tense plural or infinitive, is a possible ambiguity. In contrast, there is no word form in the lexicon
that would be ambiguous between an adjective and a comma, hence we don't want to generate such ambiguity in our symbolic sentences.

We solve the problem by looking at the lexicon again. We extract all the words and form ambiguity classes \todo{find if that's a term that should be cited/who invented it and where it was first used}: all the words that can be ambiguous with each other. However, with thousands of readings, this would 
extract all the  leaves us with so many combinations, that hardcoding wouldn't be efficient. We

\todo{List of word forms+readings; list of distinct readings; make into matrix where we see that x and y can be ambiguous but a and z never; smart things with constraints (Koen)}





Now with this, we can detect even these kinds of rules as problematic:

 \begin{itemize}
 \item[] \begin{verbatim}
 REMOVE verb IF (-1 det) ;
 REMOVE noun IF (-1 det) ;
 \end{verbatim}
 \end{itemize}

 With our symbolic sentence, these rules will be no problem; to apply the latter, we only need to construct a target that has a realistic ambiguity with a noun; the verb is gone already.
 However, in the context of real life texts, the grammar writer might actually mean the following:

 \begin{itemize}
 \item[] \begin{verbatim}
 REMOVE verb IF (-1 det) (0 noun) ;
 REMOVE noun IF (-1 det) (0 verb) ;
 \end{verbatim}
 \end{itemize}

 In that case, those rules are contradictory, and we would like to find it out. The additional constraints will prevent the SAT solver from creating an ambiguity where noun is ambiguous with e.g. punctuation.


\paragraph{Putting it all together}

Once we know how to apply one rule $R'$ to a symbolic sentence, we can apply all rules preceding the rule $R$ that is under analysis. We simply apply each rule to the result of applying the previous rule. In this way, we end up with a symbolic sentence that represents all sentences that could be the result of applying all those rules.

Finally, we can take a look at the rule $R$ we want to analyse. Here is an example:
\begin{itemize}
\item[] \texttt{REMOVE det IF (1 verb) ;}
\end{itemize}
If we take the symbolic sentence above as input, we want to ask whether or not it can trigger the rule $R$. We do this by adding some more constraints to the SAT-solver.

First, the context of the rule should be applicable, meaning that the second word should have a reading with a ``verb'' tag:
\begin{center}
\begin{tabular}{c}
$w_4' \vee w_5'$
\end{tabular}
\end{center}
Second, the rule should be able to remove the ``det'' tag, meaning that the first word should have a reading with a ``det'' tag, and there should be at least one other reading:
\begin{center}
\begin{tabular}{c}
$v_1 \wedge (v_2 \vee v_3 \vee v_4 \vee v_5)$
\end{tabular}
\end{center}
If the SAT-solver can find a solution to all constraints generated so far, we have found a concrete sentence that satisfies our goal. If the SAT-solver cannot find a solution, it means that there are no sentences that can ever trigger rule $R$. This means that there is something wrong with the grammar.

% \paragraph{Other questions we can ask}

% Apart from finding out what rules prevent others from applying, we can also find out if there is a conflict that is rule-internal, such as nonexisting tag set or contradicting requirements of a context word (e.g. a word must be unambiguously two different POS).

% In addition to analysing a whole grammar, 
% we can construct all kinds of symbolic sentences to test out individual rules. 
% We can set the length, restrict individual words (e.g. ``3rd word must be a noun''), require that it triggers some rule but not other. 
% This functionality can aid the grammar writer in the process, to see if they have missed a case or defined the tagset correctly.

